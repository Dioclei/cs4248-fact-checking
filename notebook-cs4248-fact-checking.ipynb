{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c7716c9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-12T17:10:21.932398Z",
     "iopub.status.busy": "2025-03-12T17:10:21.931912Z",
     "iopub.status.idle": "2025-03-12T17:10:21.938005Z",
     "shell.execute_reply": "2025-03-12T17:10:21.936934Z"
    },
    "papermill": {
     "duration": 0.015797,
     "end_time": "2025-03-12T17:10:21.939592",
     "exception": false,
     "start_time": "2025-03-12T17:10:21.923795",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TODO: Replace with your Student NET ID\n",
    "_NAME = \"Jason Lee Jia Xuan\"\n",
    "_STUDENT_NUM = 'E0957670'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52b3b95b",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-03-12T17:10:21.952620Z",
     "iopub.status.busy": "2025-03-12T17:10:21.952294Z",
     "iopub.status.idle": "2025-03-12T17:10:31.736828Z",
     "shell.execute_reply": "2025-03-12T17:10:31.735658Z"
    },
    "papermill": {
     "duration": 9.792842,
     "end_time": "2025-03-12T17:10:31.738423",
     "exception": false,
     "start_time": "2025-03-12T17:10:21.945581",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spaCy pipeline:  ['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']\n",
      "/kaggle/input/glove-6b/glove.6B.200d.txt\n",
      "/kaggle/input/glove-6b/glove.6B.50d.txt\n",
      "/kaggle/input/glove-6b/glove.6B.300d.txt\n",
      "/kaggle/input/glove-6b/glove.6B.100d.txt\n",
      "/kaggle/input/cs-4248-fact-checking-2420/train.csv\n",
      "/kaggle/input/cs-4248-fact-checking-2420/test.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import math\n",
    "import time\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from sklearn.metrics import f1_score\n",
    "# for tokenizing and extracting bag-of-words vectors\n",
    "# from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.exceptions import NotFittedError\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# tokenizer\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "print(\"spaCy pipeline: \", nlp.pipe_names)\n",
    "\n",
    "# multilayer perceptron\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53203c86",
   "metadata": {
    "papermill": {
     "duration": 0.005393,
     "end_time": "2025-03-12T17:10:31.749745",
     "exception": false,
     "start_time": "2025-03-12T17:10:31.744352",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "943d111f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-12T17:10:31.762844Z",
     "iopub.status.busy": "2025-03-12T17:10:31.762201Z",
     "iopub.status.idle": "2025-03-12T17:10:31.874521Z",
     "shell.execute_reply": "2025-03-12T17:10:31.873435Z"
    },
    "papermill": {
     "duration": 0.121327,
     "end_time": "2025-03-12T17:10:31.876562",
     "exception": false,
     "start_time": "2025-03-12T17:10:31.755235",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence_id</th>\n",
       "      <th>Text</th>\n",
       "      <th>Verdict</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>I think we've seen a deterioration of values.</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>I think for a while as a nation we condoned th...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>For a while, as I recall, it even seems to me ...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>So we've seen a deterioration in values, and o...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>We got away, we got into this feeling that val...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Sentence_id                                               Text  Verdict\n",
       "0            1      I think we've seen a deterioration of values.       -1\n",
       "1            2  I think for a while as a nation we condoned th...       -1\n",
       "2            3  For a while, as I recall, it even seems to me ...       -1\n",
       "3            4  So we've seen a deterioration in values, and o...       -1\n",
       "4            5  We got away, we got into this feeling that val...       -1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import data\n",
    "train_data = pd.read_csv(\"../input/cs-4248-fact-checking-2420/train.csv\")\n",
    "test_data = pd.read_csv(\"../input/cs-4248-fact-checking-2420/test.csv\")\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc606ce2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-12T17:10:31.890384Z",
     "iopub.status.busy": "2025-03-12T17:10:31.890016Z",
     "iopub.status.idle": "2025-03-12T17:11:03.725246Z",
     "shell.execute_reply": "2025-03-12T17:11:03.724017Z"
    },
    "papermill": {
     "duration": 31.849591,
     "end_time": "2025-03-12T17:11:03.732411",
     "exception": false,
     "start_time": "2025-03-12T17:10:31.882820",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400000 words loaded!\n",
      "time taken: 31.826785802841187\n"
     ]
    }
   ],
   "source": [
    "# import GloVe word embeddings\n",
    "glove_word_embeddings = {}\n",
    "word_embedding_dim = 300 # adjust as necessary\n",
    "with open(\"/kaggle/input/glove-6b/glove.6B.300d.txt\", 'r', encoding=\"utf-8\") as file:\n",
    "    start = time.time()\n",
    "    for line in file:\n",
    "        spl = line.split()\n",
    "        word = spl[0]\n",
    "        embedding = spl[1:]\n",
    "        glove_word_embeddings[word] = np.array(embedding, dtype=np.float64)\n",
    "    end = time.time()\n",
    "    print(f\"{len(glove_word_embeddings)} words loaded!\")\n",
    "    print(f\"time taken: {end - start}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f573895",
   "metadata": {
    "papermill": {
     "duration": 0.005451,
     "end_time": "2025-03-12T17:11:03.743610",
     "exception": false,
     "start_time": "2025-03-12T17:11:03.738159",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Data Preprocessing\n",
    "Do some data preprocessing so that the data is of a good quality\n",
    "- Clean data\n",
    "- Resolve imbalances\n",
    "    - Sampling\n",
    "    - Data augmentation (?)\n",
    "- Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a45d7ffe",
   "metadata": {
    "papermill": {
     "duration": 0.005589,
     "end_time": "2025-03-12T17:11:03.755055",
     "exception": false,
     "start_time": "2025-03-12T17:11:03.749466",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Clean Data\n",
    "Obtain a standardized set of data\n",
    "- Data should not contain missing values\n",
    "- Data should not have duplicates. If there are any duplicates, remove them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff27eca4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-12T17:11:03.767907Z",
     "iopub.status.busy": "2025-03-12T17:11:03.767543Z",
     "iopub.status.idle": "2025-03-12T17:11:03.773035Z",
     "shell.execute_reply": "2025-03-12T17:11:03.772005Z"
    },
    "papermill": {
     "duration": 0.013922,
     "end_time": "2025-03-12T17:11:03.774759",
     "exception": false,
     "start_time": "2025-03-12T17:11:03.760837",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# remove missing values and remove duplicates\n",
    "def clean_data(data):\n",
    "    # count missing data, I think kaggle tells us the data does not have missing values\n",
    "    print(\"Rows with null Sentence_id: \", sum(data[\"Sentence_id\"].isnull()))\n",
    "    print(\"Rows with null Text: \", sum(data[\"Text\"].isnull()))\n",
    "    print(\"Rows with null Verdict: \", sum(data[\"Verdict\"].isnull()))\n",
    "\n",
    "    # remove duplicates from the data\n",
    "    # set keep=False because we have no idea which label is actually correct\n",
    "    data_cleaned = data.drop_duplicates([\"Text\"], keep=False)\n",
    "    return data_cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4590339",
   "metadata": {
    "papermill": {
     "duration": 0.005581,
     "end_time": "2025-03-12T17:11:03.786490",
     "exception": false,
     "start_time": "2025-03-12T17:11:03.780909",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Resolve Class Imbalance\n",
    "In order to train the model properly, we need to resolve the class imbalance.\n",
    "We can either upsample or downsample.\n",
    "- For simplicity, we try downsampling here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6cbdddb4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-12T17:11:03.799593Z",
     "iopub.status.busy": "2025-03-12T17:11:03.799258Z",
     "iopub.status.idle": "2025-03-12T17:11:03.805728Z",
     "shell.execute_reply": "2025-03-12T17:11:03.804630Z"
    },
    "papermill": {
     "duration": 0.014784,
     "end_time": "2025-03-12T17:11:03.807238",
     "exception": false,
     "start_time": "2025-03-12T17:11:03.792454",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def balance_classes(data):\n",
    "    # show how many data points there are for each verdict in the training data\n",
    "    print(\"Old counts:\\n\", data.groupby(\"Verdict\").count())\n",
    "    # obtain number of samples for smallest class\n",
    "    min_count = data.groupby(\"Verdict\").count()['Text'].min()\n",
    "    # sample from all classes this amount\n",
    "    class1 = data[data['Verdict'] == -1].sample(min_count)\n",
    "    class2 = data[data['Verdict'] == 0].sample(min_count)\n",
    "    class3 = data[data['Verdict'] == 1].sample(min_count)\n",
    "    # combine\n",
    "    data_balanced = pd.concat([class1, class2, class3], ignore_index=True)\n",
    "    # verify counts\n",
    "    print(\"New counts:\\n\", data_balanced.groupby(\"Verdict\").count())\n",
    "    return data_balanced"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c689eca",
   "metadata": {
    "papermill": {
     "duration": 0.00564,
     "end_time": "2025-03-12T17:11:03.818871",
     "exception": false,
     "start_time": "2025-03-12T17:11:03.813231",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Tokenization, Case Folding, Stopword and Punctuation Removal\n",
    "Perform tokenization on text data:\n",
    "- make lowercase\n",
    "- remove stopwords\n",
    "- remove punctuation\n",
    "- possible to lemmatize but it is not done here.\n",
    "\n",
    "If any sentences only contain stopwords, then remove the whole row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "92ccd701",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-12T17:11:03.832162Z",
     "iopub.status.busy": "2025-03-12T17:11:03.831813Z",
     "iopub.status.idle": "2025-03-12T17:11:03.841036Z",
     "shell.execute_reply": "2025-03-12T17:11:03.839924Z"
    },
    "papermill": {
     "duration": 0.017909,
     "end_time": "2025-03-12T17:11:03.842847",
     "exception": false,
     "start_time": "2025-03-12T17:11:03.824938",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tokenize(data):\n",
    "    result = data.copy()\n",
    "    text = result[\"Text\"]\n",
    "    tokens = []\n",
    "    pos = []\n",
    "    remove = [] # if no tokens are generated, remove it later\n",
    "    content = []\n",
    "    nouns = []\n",
    "    verbs = []\n",
    "    other = []\n",
    "    reject_pos = set(['DET', 'NUM', 'SYM', 'INTJ', 'X'])\n",
    "    for doc in nlp.pipe(text, batch_size=50):\n",
    "        # remove tokens that we don't want\n",
    "        doc_clean = [token for token in doc if not token.is_punct and token.pos_ not in reject_pos]\n",
    "        # tokenize\n",
    "        t = np.array([token.lower_ for token in doc_clean])\n",
    "        p = np.array([token.pos_ for token in doc_clean])\n",
    "        x = np.array([token.lemma_.lower() for token in doc_clean if not token.is_stop]) # lemmatized, stopword removed token list\n",
    "        # separate nouns, verbs, and others to extract meaning better\n",
    "        n = np.array([token.lower_ for token in doc_clean if token.pos_ == 'NOUN'])\n",
    "        v = np.array([token.lower_ for token in doc_clean if token.pos_ == 'VERB'])\n",
    "        o = np.array([token.lower_ for token in doc_clean if not token.pos_ == 'VERB' and not token.pos_ == 'NOUN'])\n",
    "        remove.append(t.shape[0] == 0)\n",
    "        tokens.append(t)\n",
    "        content.append(x)\n",
    "        pos.append(p)\n",
    "        nouns.append(n)\n",
    "        verbs.append(v)\n",
    "        other.append(o)\n",
    "\n",
    "    result[\"Tokens\"] = tokens\n",
    "    result[\"Pos\"] = pos\n",
    "    result[\"Remove\"] = remove\n",
    "    result[\"Content\"] = content\n",
    "    result[\"Nouns\"], result[\"Verbs\"], result[\"Others\"] = nouns, verbs, other\n",
    "    result = result.drop(result[result[\"Remove\"]].index)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ecc82f6",
   "metadata": {
    "papermill": {
     "duration": 0.005819,
     "end_time": "2025-03-12T17:11:03.855013",
     "exception": false,
     "start_time": "2025-03-12T17:11:03.849194",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Data Split\n",
    "Split data into training, validation, and test sets for training a model.\n",
    "We will use a 80-10-10 split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "64a47bc8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-12T17:11:03.868817Z",
     "iopub.status.busy": "2025-03-12T17:11:03.868376Z",
     "iopub.status.idle": "2025-03-12T17:11:03.876592Z",
     "shell.execute_reply": "2025-03-12T17:11:03.875243Z"
    },
    "papermill": {
     "duration": 0.017168,
     "end_time": "2025-03-12T17:11:03.878427",
     "exception": false,
     "start_time": "2025-03-12T17:11:03.861259",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def split_data(tokenized_data):\n",
    "    # split by class first\n",
    "    class1 = tokenized_data[tokenized_data[\"Verdict\"] == -1]\n",
    "    class2 = tokenized_data[tokenized_data[\"Verdict\"] == 1]\n",
    "    class3 = tokenized_data[tokenized_data[\"Verdict\"] == 0]\n",
    "    \n",
    "    # split tokenized data into 80-20 split before engineering features\n",
    "    count80 = math.floor(class1.shape[0] * 0.8)\n",
    "    sample1a = class1.sample(count80)\n",
    "    class1b = class1.drop(sample1a.index)\n",
    "    count10 = math.floor(class1b.shape[0] * 0.8)\n",
    "    sample1b = class1b.sample(count10)\n",
    "    sample1c = class1b.drop(sample1b.index)\n",
    "    \n",
    "    sample2a = class2.sample(count80)\n",
    "    class2b = class2.drop(sample2a.index)\n",
    "    sample2b = class2b.sample(count10)\n",
    "    sample2c = class2b.drop(sample2b.index)\n",
    "    \n",
    "    sample3a = class3.sample(count80)\n",
    "    class3b = class3.drop(sample3a.index)\n",
    "    sample3b = class3b.sample(count10)\n",
    "    sample3c = class3b.drop(sample3b.index)\n",
    "\n",
    "    \n",
    "    tokenized_train = pd.concat((sample1a, sample2a, sample3a), axis=0)\n",
    "    tokenized_valid = pd.concat((sample1b, sample2b, sample3b), axis=0)\n",
    "    tokenized_test = pd.concat((sample1c, sample2c, sample3c), axis=0)\n",
    "    \n",
    "    y_train = tokenized_train[\"Verdict\"]\n",
    "    y_valid = tokenized_valid[\"Verdict\"]\n",
    "    y_test = tokenized_test[\"Verdict\"]\n",
    "    return tokenized_train, tokenized_valid, tokenized_test, y_train, y_valid, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e275ce5c",
   "metadata": {
    "papermill": {
     "duration": 0.00553,
     "end_time": "2025-03-12T17:11:03.889946",
     "exception": false,
     "start_time": "2025-03-12T17:11:03.884416",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Feature Engineering\n",
    "After processing the Text into tokens, we have to derive features from the tokens. A few approaches available:\n",
    "- Bag-of-Words representation\n",
    "- Document term matrix with tf-idf weights\n",
    "- PPMI term context matrix (?)\n",
    "- Dense word embedding (Word2Vec)\n",
    "- Can also apply PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c261892c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-12T17:11:03.903201Z",
     "iopub.status.busy": "2025-03-12T17:11:03.902643Z",
     "iopub.status.idle": "2025-03-12T17:11:03.913464Z",
     "shell.execute_reply": "2025-03-12T17:11:03.912329Z"
    },
    "papermill": {
     "duration": 0.019505,
     "end_time": "2025-03-12T17:11:03.915337",
     "exception": false,
     "start_time": "2025-03-12T17:11:03.895832",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def count(doc, words):\n",
    "    count = 0\n",
    "    for token in doc:\n",
    "        if token in words:\n",
    "            count += 1\n",
    "    return count\n",
    "\n",
    "def count_ref_other_people(doc):\n",
    "    words = set([\"you\", \"others\", \"people\", \"yourself\", \"yourselves\", \"your\", \"\"])\n",
    "    return count(doc, words)\n",
    "\n",
    "def count_ref_self(doc):\n",
    "    words = set([\"i\", \"self\", \"me\", \"myself\", \"mine\", \"friends\", \"friend\", \"family\", \"buddy\", \"mate\"])\n",
    "    return count(doc, words)\n",
    "\n",
    "def count_causal(doc):\n",
    "    words = set([\"cause\", \"because\", \"effect\", \"hence\", \"therefore\", \"thus\", \"since\", \"reason\", \"due\"])\n",
    "    return count(doc, words)\n",
    "\n",
    "def count_negation(doc):\n",
    "    words = set([\"no\", \"not\", \"neither\", \"none\", \"nobody\", \"nothing\", \"nowhere\", \"hardly\", \"seldom\", \"little\"])\n",
    "    return count(doc, words)\n",
    "\n",
    "def count_promise(doc):\n",
    "    words = set([\"\\'ll\", \"will\", \"should\", \"future\", \"believe\", \"think\", \"consider\", \"propose\", \"want\", \"suspect\", \"suppose\", \"time\", \"come\", \"upcoming\"])\n",
    "    return count(doc, words)\n",
    "\n",
    "def count_all_or_nothing(doc):\n",
    "    words = set([\"everything\", \"nothing\", \"everyone\", \"all\", \"no\", \"never\", \"always\", ])\n",
    "    return count(doc, words)\n",
    "\n",
    "def count_prosocial(doc):\n",
    "    words = set([\"care\", \"help\", \"please\", \"thank\", \"thanks\", \"support\", \"trust\", \"faith\"])\n",
    "    return count(doc, words)\n",
    "\n",
    "def count_together(doc):\n",
    "    words = set([\"we\", \"us\", \"our\", \"ours\", \"together\"])\n",
    "    return count(doc, words)\n",
    "\n",
    "def count_accusation(doc):\n",
    "    words = set([\"lie\", \"steal\", \"cheat\", \"betray\", \"deceive\", \"manipulate\", \"harmed\", \"ruin\", \"destroy\", \"fake\", \"liar\", \"cheater\", \"fraud\", \"backstabber\", \"traitor\", \"thief\", \"hypocrite\", \"coward\", \"fool\", \"idiot\", \"moron\"])\n",
    "    return count(doc, words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "babbe6e6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-12T17:11:03.928640Z",
     "iopub.status.busy": "2025-03-12T17:11:03.928242Z",
     "iopub.status.idle": "2025-03-12T17:11:03.938148Z",
     "shell.execute_reply": "2025-03-12T17:11:03.936972Z"
    },
    "papermill": {
     "duration": 0.018721,
     "end_time": "2025-03-12T17:11:03.939994",
     "exception": false,
     "start_time": "2025-03-12T17:11:03.921273",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# obtain an embedding vector representing each sentence by taking sum over all word embeddings in each sentence\n",
    "def compute_embeddings(corpus):\n",
    "    doc_embeddings = []\n",
    "    for doc in corpus:\n",
    "        doc_embedding = np.zeros(word_embedding_dim, dtype=np.float64)\n",
    "        for token in doc:\n",
    "            if token in glove_word_embeddings:\n",
    "                doc_embedding = np.add(doc_embedding, glove_word_embeddings[token])\n",
    "        doc_embeddings.append(doc_embedding)\n",
    "    return np.array(doc_embeddings)\n",
    "\n",
    "def compute_tfidf(tokens, vectorizer):\n",
    "    try:\n",
    "        vectorizer.transform(tokens)\n",
    "    except NotFittedError as e:\n",
    "        print(\"fitting vectorizer!\")\n",
    "        vectorizer.fit(tokens)\n",
    "\n",
    "    return vectorizer.transform(tokens).toarray()\n",
    "\n",
    "# compute counts of certain words\n",
    "def get_additional_features(corpus):\n",
    "    result = []\n",
    "    for doc in corpus:\n",
    "        counts = [\n",
    "            count_ref_other_people(doc),\n",
    "            count_ref_self(doc),\n",
    "            count_causal(doc),\n",
    "            count_negation(doc),\n",
    "            count_promise(doc),\n",
    "            count_all_or_nothing(doc),\n",
    "            count_prosocial(doc),\n",
    "            count_together(doc),\n",
    "        ]\n",
    "        result.append(counts)\n",
    "    return np.array(result)\n",
    "\n",
    "# some counts need lemmatized vocabulary\n",
    "def get_additional_features2(lemmas):\n",
    "    result = []\n",
    "    for doc in lemmas:\n",
    "        counts = [\n",
    "            count_accusation(doc)\n",
    "        ]\n",
    "        result.append(counts)\n",
    "    return np.array(result)\n",
    "\n",
    "def conduct_pca(features, pca):\n",
    "    try:\n",
    "        pca.transform(features)\n",
    "    except NotFittedError as e:\n",
    "        print(\"fitting pca!\")\n",
    "        pca.fit(features)\n",
    "        print(\"PCA cumulative variance: \", np.cumsum(pca.explained_variance_ratio_))\n",
    "    return pca.transform(features)\n",
    "    \n",
    "\n",
    "# pipeline which outputs all features, run with train data first\n",
    "def get_features(tokenized_data, model):\n",
    "    sparse = [\n",
    "        compute_tfidf(tokenized_data[\"Pos\"], model.pos_vectorizer),\n",
    "        get_additional_features(tokenized_data[\"Tokens\"]),\n",
    "        get_additional_features2(tokenized_data[\"Content\"])\n",
    "    ]\n",
    "    sparse = np.concatenate(sparse, axis=1)\n",
    "    sparse_pca = conduct_pca(sparse, model.pca)\n",
    "    features = [\n",
    "        compute_embeddings(tokenized_data[\"Nouns\"]),\n",
    "        compute_embeddings(tokenized_data[\"Verbs\"]),\n",
    "        compute_embeddings(tokenized_data[\"Others\"]),\n",
    "        sparse_pca\n",
    "    ]\n",
    "    features = np.concatenate(features, axis=1)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "272463f1",
   "metadata": {
    "papermill": {
     "duration": 0.005856,
     "end_time": "2025-03-12T17:11:03.952169",
     "exception": false,
     "start_time": "2025-03-12T17:11:03.946313",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Modelling\n",
    "For the model, we can choose from these 3 approaches:\n",
    "- Naive Bayes (generative classifier)\n",
    "- Logistic Regression (discriminative classifier)\n",
    "- Multi-Layer Perceptron Neural Network (discriminative classifier)\n",
    "\n",
    "To obtain a baseline model, we will only do this for now:\n",
    "- Features: Bag-of-Words, one-hot encoding of documents\n",
    "- Model: Naive Bayes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "40c1d391",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-12T17:11:03.965358Z",
     "iopub.status.busy": "2025-03-12T17:11:03.964964Z",
     "iopub.status.idle": "2025-03-12T17:11:03.971449Z",
     "shell.execute_reply": "2025-03-12T17:11:03.970211Z"
    },
    "papermill": {
     "duration": 0.015126,
     "end_time": "2025-03-12T17:11:03.973203",
     "exception": false,
     "start_time": "2025-03-12T17:11:03.958077",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Neural Network works on word embeddings of sentence and other features\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.pos_vectorizer = TfidfVectorizer(ngram_range=(1, 1), lowercase=False, tokenizer=lambda x:x, token_pattern=None)\n",
    "        self.pca = PCA(n_components=15)\n",
    "\n",
    "        # define multilayer perceptron layers\n",
    "        self.fc1 = nn.Linear(915, 256)\n",
    "        self.fc2 = nn.Linear(256, 64)\n",
    "        self.fc3 = nn.Linear(64, 3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f88375a2",
   "metadata": {
    "papermill": {
     "duration": 0.005559,
     "end_time": "2025-03-12T17:11:03.984757",
     "exception": false,
     "start_time": "2025-03-12T17:11:03.979198",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Results\n",
    "Predict results and compute performance of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5ad2f08d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-12T17:11:03.997704Z",
     "iopub.status.busy": "2025-03-12T17:11:03.997300Z",
     "iopub.status.idle": "2025-03-12T17:11:04.004639Z",
     "shell.execute_reply": "2025-03-12T17:11:04.003487Z"
    },
    "papermill": {
     "duration": 0.0159,
     "end_time": "2025-03-12T17:11:04.006448",
     "exception": false,
     "start_time": "2025-03-12T17:11:03.990548",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_performance_per_class(model, X_test, y_test):\n",
    "    # y_pred = model.predict(X_test)\n",
    "    pred_model = model(X_test_tensor)\n",
    "    _, y_pred = pred_model.max(1)\n",
    "    # need to convert values of 2 in y_pred back to -1\n",
    "    y_pred = y_pred.numpy()\n",
    "    y_test = y_test.numpy()\n",
    "    y_pred[y_pred == 2] = -1\n",
    "    y_test[y_test == 2] = -1\n",
    "    print(y_pred)\n",
    "    print(y_pred.shape)\n",
    "    print(y_test.shape)\n",
    "    # compute separately for each class\n",
    "    result = []\n",
    "    for c in [-1, 0, 1]:\n",
    "        TP = np.sum((y_pred == c) & (y_test == c))\n",
    "        FP = np.sum((y_pred == c) & (y_test != c))\n",
    "        FN = np.sum((y_pred != c) & (y_test == c))\n",
    "        TN = np.sum((y_pred != c) & (y_test != c))\n",
    "        precision = TP / (TP + FP)\n",
    "        recall = TP / (TP + FN)\n",
    "        F1 = 2 * (precision * recall) / (precision + recall)\n",
    "        result.append([c, precision, recall, F1])\n",
    "    return pd.DataFrame(data=np.array(result), columns=[\"Class\", \"Precision\", \"Recall\", \"F1\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9367bb3b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-12T17:11:04.019772Z",
     "iopub.status.busy": "2025-03-12T17:11:04.019372Z",
     "iopub.status.idle": "2025-03-12T17:11:04.023811Z",
     "shell.execute_reply": "2025-03-12T17:11:04.022744Z"
    },
    "papermill": {
     "duration": 0.013068,
     "end_time": "2025-03-12T17:11:04.025534",
     "exception": false,
     "start_time": "2025-03-12T17:11:04.012466",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_macro_f1(f1_scores):\n",
    "    return np.mean(f1_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d615482",
   "metadata": {
    "papermill": {
     "duration": 0.005575,
     "end_time": "2025-03-12T17:11:04.037104",
     "exception": false,
     "start_time": "2025-03-12T17:11:04.031529",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Run Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4c7c891f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-12T17:11:04.050468Z",
     "iopub.status.busy": "2025-03-12T17:11:04.050067Z",
     "iopub.status.idle": "2025-03-12T17:11:27.498262Z",
     "shell.execute_reply": "2025-03-12T17:11:27.496903Z"
    },
    "papermill": {
     "duration": 23.45723,
     "end_time": "2025-03-12T17:11:27.500343",
     "exception": false,
     "start_time": "2025-03-12T17:11:04.043113",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows with null Sentence_id:  0\n",
      "Rows with null Text:  0\n",
      "Rows with null Verdict:  0\n",
      "Old counts:\n",
      "          Sentence_id   Text\n",
      "Verdict                    \n",
      "-1             14542  14542\n",
      " 0              2388   2388\n",
      " 1              5386   5386\n",
      "New counts:\n",
      "          Sentence_id  Text\n",
      "Verdict                   \n",
      "-1              2388  2388\n",
      " 0              2388  2388\n",
      " 1              2388  2388\n",
      "Tokenized data columns:  Index(['Sentence_id', 'Text', 'Verdict', 'Tokens', 'Pos', 'Remove', 'Content',\n",
      "       'Nouns', 'Verbs', 'Others'],\n",
      "      dtype='object')\n",
      "fitting vectorizer!\n",
      "fitting pca!\n",
      "PCA cumulative variance:  [0.25548436 0.49927365 0.66060533 0.748518   0.790671   0.82367708\n",
      " 0.85273674 0.87994497 0.90355741 0.92404808 0.93796182 0.94987535\n",
      " 0.96031001 0.96929691 0.9780928 ]\n",
      "X_train:  (5730, 915)\n"
     ]
    }
   ],
   "source": [
    "# clean data\n",
    "cleaned_data = clean_data(train_data)\n",
    "# balanced data\n",
    "balanced_data = balance_classes(cleaned_data)\n",
    "# tokenize data\n",
    "tokenized_data = tokenize(balanced_data)\n",
    "print(\"Tokenized data columns: \", tokenized_data.columns)\n",
    "# split data\n",
    "tokenized_train, tokenized_valid, tokenized_test, y_train, y_valid, y_test = split_data(tokenized_data)\n",
    "# engineer features\n",
    "model = Model()\n",
    "X_train = get_features(tokenized_train, model)\n",
    "print(\"X_train: \", X_train.shape)\n",
    "X_valid = get_features(tokenized_valid, model)\n",
    "X_test = get_features(tokenized_test, model)\n",
    "\n",
    "# convert data to tensors\n",
    "y_train = y_train.replace(to_replace=-1, value=2)\n",
    "y_valid = y_valid.replace(to_replace=-1, value=2)\n",
    "y_test = y_test.replace(to_replace=-1, value=2)\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train.to_numpy(), dtype=torch.long)\n",
    "X_valid_tensor = torch.tensor(X_valid, dtype=torch.float32)\n",
    "y_valid_tensor = torch.tensor(y_valid.to_numpy(), dtype=torch.long)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test.to_numpy(), dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d07ed7a4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-12T17:11:27.514534Z",
     "iopub.status.busy": "2025-03-12T17:11:27.514068Z",
     "iopub.status.idle": "2025-03-12T17:16:30.736099Z",
     "shell.execute_reply": "2025-03-12T17:16:30.734908Z"
    },
    "papermill": {
     "duration": 303.23166,
     "end_time": "2025-03-12T17:16:30.738460",
     "exception": false,
     "start_time": "2025-03-12T17:11:27.506800",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1140661239624023 1.1183764934539795\n",
      "1.0321420431137085 1.033351182937622\n",
      "0.9784773588180542 0.9835978150367737\n",
      "0.938242495059967 0.9483328461647034\n",
      "0.9062995910644531 0.9209111332893372\n",
      "0.8805805444717407 0.8993020057678223\n",
      "0.8594525456428528 0.8822794556617737\n",
      "0.8418290019035339 0.8687795400619507\n",
      "0.8266236186027527 0.8576414585113525\n",
      "0.8131849765777588 0.8484243154525757\n"
     ]
    }
   ],
   "source": [
    "# train neural network\n",
    "\n",
    "# define model parameters\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
    "\n",
    "num_epochs = 5000\n",
    "for n in range(num_epochs):\n",
    "    y_pred = model(X_train_tensor)\n",
    "    train_loss = loss_fn(y_pred, y_train_tensor)\n",
    "    optimizer.zero_grad()\n",
    "    train_loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # validate the loss to make sure it doesn't overfit\n",
    "    y_pred = model(X_valid_tensor)\n",
    "    valid_loss = loss_fn(y_pred, y_valid_tensor)\n",
    "    if n % 500 == 0:\n",
    "        print(train_loss.item(), valid_loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eb7ffbef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-12T17:16:30.754498Z",
     "iopub.status.busy": "2025-03-12T17:16:30.753761Z",
     "iopub.status.idle": "2025-03-12T17:16:30.774217Z",
     "shell.execute_reply": "2025-03-12T17:16:30.772944Z"
    },
    "papermill": {
     "duration": 0.030474,
     "end_time": "2025-03-12T17:16:30.776153",
     "exception": false,
     "start_time": "2025-03-12T17:16:30.745679",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1  1  0  0  1  0 -1 -1  0 -1 -1 -1  0 -1 -1  0  1 -1 -1 -1  1 -1  0 -1\n",
      " -1 -1 -1 -1 -1  0 -1  0  0 -1 -1  1 -1 -1 -1 -1 -1  0 -1 -1 -1 -1  0  1\n",
      " -1 -1 -1 -1 -1  1 -1 -1  0 -1 -1 -1 -1 -1 -1 -1  0 -1 -1  0 -1 -1  1 -1\n",
      " -1 -1 -1 -1 -1  0 -1  0 -1 -1 -1 -1 -1  1  0  0 -1 -1  0 -1 -1  1  0 -1\n",
      "  1  1  0 -1  1  1  1  1  1  0  1 -1  1  0  0  1  1  1  0  1  1  1  1  1\n",
      "  0  1  1  0  1  1  1 -1  0  1  1  1 -1  1  0  1  1 -1  0  1 -1  1  1 -1\n",
      "  1  1  1  0  1  1  1  0  1 -1  1  1  1  0  1  0  0 -1  1  1  0  1  0  0\n",
      " -1  1  1  1  0  1  1 -1  0  1  1  0  1  0  1  1  0  1  1  1  1  1  1  1\n",
      "  0  0  0  1  0 -1  0  0  0  0  0  1 -1  1 -1  0  0  0 -1  1  1  0  1  0\n",
      "  0  0  0  0  0  0  0  1  0  0  0  0  1 -1  0  0  0 -1  1  0  0  0  0  0\n",
      "  0  1  0 -1  0 -1  0  1  0  0  0  0  0  0  0  0  0  0 -1 -1  1 -1  0  1\n",
      "  0  1  0  1  0 -1  0  1  1  0  0  0  0 -1  1  1  0 -1  0 -1  0  0  0  0]\n",
      "(288,)\n",
      "(288,)\n",
      "   Class  Precision    Recall        F1\n",
      "0   -1.0   0.714286  0.677083  0.695187\n",
      "1    0.0   0.584906  0.645833  0.613861\n",
      "2    1.0   0.681319  0.645833  0.663102\n",
      "Macro F1:  0.6573833853973633\n"
     ]
    }
   ],
   "source": [
    "# compute results\n",
    "results = compute_performance_per_class(model, X_test_tensor, y_test_tensor)\n",
    "print(results)\n",
    "macro_f1 = compute_macro_f1(results['F1'])\n",
    "print(\"Macro F1: \", macro_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c197b54",
   "metadata": {
    "papermill": {
     "duration": 0.006308,
     "end_time": "2025-03-12T17:16:30.789205",
     "exception": false,
     "start_time": "2025-03-12T17:16:30.782897",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b066da25",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-12T17:16:30.804361Z",
     "iopub.status.busy": "2025-03-12T17:16:30.803998Z",
     "iopub.status.idle": "2025-03-12T17:16:33.806598Z",
     "shell.execute_reply": "2025-03-12T17:16:33.805370Z"
    },
    "papermill": {
     "duration": 3.012491,
     "end_time": "2025-03-12T17:16:33.808398",
     "exception": false,
     "start_time": "2025-03-12T17:16:30.795907",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1032, 2)\n",
      "Tokenized data columns:  Index(['Sentence_id', 'Text', 'Tokens', 'Pos', 'Remove', 'Content', 'Nouns',\n",
      "       'Verbs', 'Others'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "def generate_result(test, y_pred, filename):\n",
    "    # clean data\n",
    "    # cleaned_data = clean_data(test)\n",
    "    # balanced data\n",
    "    # balanced_data = balance_classes(cleaned_data)\n",
    "    # tokenize data\n",
    "    print(np.array(test).shape)\n",
    "    tokenized_data = tokenize(test)\n",
    "    print(\"Tokenized data columns: \", tokenized_data.columns)\n",
    "    X_test = get_features(tokenized_data, model)\n",
    "    X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "\n",
    "    pred_model = model(X_test_tensor)\n",
    "    _, y_pred = pred_model.max(1)\n",
    "    y_pred = y_pred.numpy()\n",
    "    y_pred[y_pred == 2] = -1\n",
    "    \n",
    "    ''' generate csv file base on the y_pred '''\n",
    "    test['Verdict'] = pd.Series(y_pred)\n",
    "    test.drop(columns=['Text'], inplace=True)\n",
    "    test.to_csv(filename, index=False)\n",
    "\n",
    "output_filename = f\"A2_{_NAME}_{_STUDENT_NUM}.csv\"\n",
    "generate_result(test_data, y_pred, output_filename)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 11087755,
     "sourceId": 93118,
     "sourceType": "competition"
    },
    {
     "datasetId": 6847560,
     "sourceId": 10999989,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30918,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 377.80527,
   "end_time": "2025-03-12T17:16:36.909385",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-03-12T17:10:19.104115",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
