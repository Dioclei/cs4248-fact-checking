{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f72dad3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-12T14:54:34.568795Z",
     "iopub.status.busy": "2025-03-12T14:54:34.568249Z",
     "iopub.status.idle": "2025-03-12T14:54:34.574806Z",
     "shell.execute_reply": "2025-03-12T14:54:34.573760Z"
    },
    "papermill": {
     "duration": 0.017415,
     "end_time": "2025-03-12T14:54:34.576821",
     "exception": false,
     "start_time": "2025-03-12T14:54:34.559406",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TODO: Replace with your Student NET ID\n",
    "_NAME = \"Jason Lee Jia Xuan\"\n",
    "_STUDENT_NUM = 'E0957670'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6fc58fd7",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-03-12T14:54:34.590086Z",
     "iopub.status.busy": "2025-03-12T14:54:34.589669Z",
     "iopub.status.idle": "2025-03-12T14:54:44.823172Z",
     "shell.execute_reply": "2025-03-12T14:54:44.821748Z"
    },
    "papermill": {
     "duration": 10.242157,
     "end_time": "2025-03-12T14:54:44.825128",
     "exception": false,
     "start_time": "2025-03-12T14:54:34.582971",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spaCy pipeline:  ['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']\n",
      "/kaggle/input/glove-6b/glove.6B.200d.txt\n",
      "/kaggle/input/glove-6b/glove.6B.50d.txt\n",
      "/kaggle/input/glove-6b/glove.6B.300d.txt\n",
      "/kaggle/input/glove-6b/glove.6B.100d.txt\n",
      "/kaggle/input/cs-4248-fact-checking-2420/train.csv\n",
      "/kaggle/input/cs-4248-fact-checking-2420/test.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import math\n",
    "import time\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from sklearn.metrics import f1_score\n",
    "# for tokenizing and extracting bag-of-words vectors\n",
    "# from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.exceptions import NotFittedError\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# tokenizer\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "print(\"spaCy pipeline: \", nlp.pipe_names)\n",
    "\n",
    "# multilayer perceptron\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc39125",
   "metadata": {
    "papermill": {
     "duration": 0.005863,
     "end_time": "2025-03-12T14:54:44.837635",
     "exception": false,
     "start_time": "2025-03-12T14:54:44.831772",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6014bf5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-12T14:54:44.850557Z",
     "iopub.status.busy": "2025-03-12T14:54:44.849849Z",
     "iopub.status.idle": "2025-03-12T14:54:44.969155Z",
     "shell.execute_reply": "2025-03-12T14:54:44.967494Z"
    },
    "papermill": {
     "duration": 0.128333,
     "end_time": "2025-03-12T14:54:44.971627",
     "exception": false,
     "start_time": "2025-03-12T14:54:44.843294",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence_id</th>\n",
       "      <th>Text</th>\n",
       "      <th>Verdict</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>I think we've seen a deterioration of values.</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>I think for a while as a nation we condoned th...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>For a while, as I recall, it even seems to me ...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>So we've seen a deterioration in values, and o...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>We got away, we got into this feeling that val...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Sentence_id                                               Text  Verdict\n",
       "0            1      I think we've seen a deterioration of values.       -1\n",
       "1            2  I think for a while as a nation we condoned th...       -1\n",
       "2            3  For a while, as I recall, it even seems to me ...       -1\n",
       "3            4  So we've seen a deterioration in values, and o...       -1\n",
       "4            5  We got away, we got into this feeling that val...       -1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import data\n",
    "train_data = pd.read_csv(\"../input/cs-4248-fact-checking-2420/train.csv\")\n",
    "test_data = pd.read_csv(\"../input/cs-4248-fact-checking-2420/test.csv\")\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e15a4ba",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-12T14:54:44.984784Z",
     "iopub.status.busy": "2025-03-12T14:54:44.984444Z",
     "iopub.status.idle": "2025-03-12T14:55:18.030053Z",
     "shell.execute_reply": "2025-03-12T14:55:18.028486Z"
    },
    "papermill": {
     "duration": 33.060937,
     "end_time": "2025-03-12T14:55:18.038649",
     "exception": false,
     "start_time": "2025-03-12T14:54:44.977712",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400000 words loaded!\n",
      "time taken: 33.03527069091797\n"
     ]
    }
   ],
   "source": [
    "# import GloVe word embeddings\n",
    "glove_word_embeddings = {}\n",
    "word_embedding_dim = 300 # adjust as necessary\n",
    "with open(\"/kaggle/input/glove-6b/glove.6B.300d.txt\", 'r', encoding=\"utf-8\") as file:\n",
    "    start = time.time()\n",
    "    for line in file:\n",
    "        spl = line.split()\n",
    "        word = spl[0]\n",
    "        embedding = spl[1:]\n",
    "        glove_word_embeddings[word] = np.array(embedding, dtype=np.float64)\n",
    "    end = time.time()\n",
    "    print(f\"{len(glove_word_embeddings)} words loaded!\")\n",
    "    print(f\"time taken: {end - start}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ec6e618",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-12T14:55:18.054744Z",
     "iopub.status.busy": "2025-03-12T14:55:18.054363Z",
     "iopub.status.idle": "2025-03-12T14:55:18.095481Z",
     "shell.execute_reply": "2025-03-12T14:55:18.094150Z"
    },
    "papermill": {
     "duration": 0.052369,
     "end_time": "2025-03-12T14:55:18.098045",
     "exception": false,
     "start_time": "2025-03-12T14:55:18.045676",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['PRON', 'AUX', 'PART', 'PRON', 'NOUN']\n"
     ]
    }
   ],
   "source": [
    "tokens = nlp(\"I am not your friend\")\n",
    "print([token.pos_ for token in tokens])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad4b82d",
   "metadata": {
    "papermill": {
     "duration": 0.006376,
     "end_time": "2025-03-12T14:55:18.111103",
     "exception": false,
     "start_time": "2025-03-12T14:55:18.104727",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Data Preprocessing\n",
    "Do some data preprocessing so that the data is of a good quality\n",
    "- Clean data\n",
    "- Resolve imbalances\n",
    "    - Sampling\n",
    "    - Data augmentation (?)\n",
    "- Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b731a6",
   "metadata": {
    "papermill": {
     "duration": 0.005813,
     "end_time": "2025-03-12T14:55:18.123055",
     "exception": false,
     "start_time": "2025-03-12T14:55:18.117242",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Clean Data\n",
    "Obtain a standardized set of data\n",
    "- Data should not contain missing values\n",
    "- Data should not have duplicates. If there are any duplicates, remove them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6f85facc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-12T14:55:18.137099Z",
     "iopub.status.busy": "2025-03-12T14:55:18.136670Z",
     "iopub.status.idle": "2025-03-12T14:55:18.142502Z",
     "shell.execute_reply": "2025-03-12T14:55:18.141282Z"
    },
    "papermill": {
     "duration": 0.015147,
     "end_time": "2025-03-12T14:55:18.144403",
     "exception": false,
     "start_time": "2025-03-12T14:55:18.129256",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# remove missing values and remove duplicates\n",
    "def clean_data(data):\n",
    "    # count missing data, I think kaggle tells us the data does not have missing values\n",
    "    print(\"Rows with null Sentence_id: \", sum(data[\"Sentence_id\"].isnull()))\n",
    "    print(\"Rows with null Text: \", sum(data[\"Text\"].isnull()))\n",
    "    print(\"Rows with null Verdict: \", sum(data[\"Verdict\"].isnull()))\n",
    "\n",
    "    # remove duplicates from the data\n",
    "    # set keep=False because we have no idea which label is actually correct\n",
    "    data_cleaned = data.drop_duplicates([\"Text\"], keep=False)\n",
    "    return data_cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba50d21",
   "metadata": {
    "papermill": {
     "duration": 0.005751,
     "end_time": "2025-03-12T14:55:18.156746",
     "exception": false,
     "start_time": "2025-03-12T14:55:18.150995",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Resolve Class Imbalance\n",
    "In order to train the model properly, we need to resolve the class imbalance.\n",
    "We can either upsample or downsample.\n",
    "- For simplicity, we try downsampling here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a2c748f6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-12T14:55:18.170161Z",
     "iopub.status.busy": "2025-03-12T14:55:18.169676Z",
     "iopub.status.idle": "2025-03-12T14:55:18.177092Z",
     "shell.execute_reply": "2025-03-12T14:55:18.175368Z"
    },
    "papermill": {
     "duration": 0.016636,
     "end_time": "2025-03-12T14:55:18.179287",
     "exception": false,
     "start_time": "2025-03-12T14:55:18.162651",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def balance_classes(data):\n",
    "    # show how many data points there are for each verdict in the training data\n",
    "    print(\"Old counts:\\n\", data.groupby(\"Verdict\").count())\n",
    "    # obtain number of samples for smallest class\n",
    "    min_count = data.groupby(\"Verdict\").count()['Text'].min()\n",
    "    # sample from all classes this amount\n",
    "    class1 = data[data['Verdict'] == -1].sample(min_count)\n",
    "    class2 = data[data['Verdict'] == 0].sample(min_count)\n",
    "    class3 = data[data['Verdict'] == 1].sample(min_count)\n",
    "    # combine\n",
    "    data_balanced = pd.concat([class1, class2, class3], ignore_index=True)\n",
    "    # verify counts\n",
    "    print(\"New counts:\\n\", data_balanced.groupby(\"Verdict\").count())\n",
    "    return data_balanced"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d150fd6",
   "metadata": {
    "papermill": {
     "duration": 0.005737,
     "end_time": "2025-03-12T14:55:18.192157",
     "exception": false,
     "start_time": "2025-03-12T14:55:18.186420",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Tokenization, Case Folding, Stopword and Punctuation Removal\n",
    "Perform tokenization on text data:\n",
    "- make lowercase\n",
    "- remove stopwords\n",
    "- remove punctuation\n",
    "- possible to lemmatize but it is not done here.\n",
    "\n",
    "If any sentences only contain stopwords, then remove the whole row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "11c96c89",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-12T14:55:18.205332Z",
     "iopub.status.busy": "2025-03-12T14:55:18.204892Z",
     "iopub.status.idle": "2025-03-12T14:55:18.213350Z",
     "shell.execute_reply": "2025-03-12T14:55:18.211843Z"
    },
    "papermill": {
     "duration": 0.017726,
     "end_time": "2025-03-12T14:55:18.215760",
     "exception": false,
     "start_time": "2025-03-12T14:55:18.198034",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tokenize(data):\n",
    "    result = data.copy()\n",
    "    text = result[\"Text\"]\n",
    "    tokens = []\n",
    "    pos = []\n",
    "    remove = [] # if no tokens are generated, remove it later\n",
    "    content = []\n",
    "    reject_pos = set(['DET', 'NUM', 'SYM', 'INTJ', 'X'])\n",
    "    for doc in nlp.pipe(text, batch_size=50):\n",
    "        # remove tokens that we don't want\n",
    "        doc_clean = [token for token in doc if not token.is_punct and token.pos_ not in reject_pos]\n",
    "        # tokenize\n",
    "        t = np.array([token.lower_ for token in doc_clean])\n",
    "        p = np.array([token.pos_ for token in doc_clean])\n",
    "        x = np.array([token.lemma_.lower() for token in doc_clean if not token.is_stop]) # lemmatized, stopword removed token list\n",
    "        remove.append(t.shape[0] == 0)\n",
    "        tokens.append(t)\n",
    "        content.append(x)\n",
    "        pos.append(p)\n",
    "\n",
    "    result[\"Tokens\"] = tokens\n",
    "    result[\"Pos\"] = pos\n",
    "    result[\"Remove\"] = remove\n",
    "    result[\"Content\"] = content\n",
    "    result = result.drop(result[result[\"Remove\"]].index)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44c00ab4",
   "metadata": {
    "papermill": {
     "duration": 0.007213,
     "end_time": "2025-03-12T14:55:18.229387",
     "exception": false,
     "start_time": "2025-03-12T14:55:18.222174",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Data Split\n",
    "Split data into training, validation, and test sets for training a model.\n",
    "We will use a 80-10-10 split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "68043da0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-12T14:55:18.244718Z",
     "iopub.status.busy": "2025-03-12T14:55:18.244328Z",
     "iopub.status.idle": "2025-03-12T14:55:18.252210Z",
     "shell.execute_reply": "2025-03-12T14:55:18.251058Z"
    },
    "papermill": {
     "duration": 0.018394,
     "end_time": "2025-03-12T14:55:18.254587",
     "exception": false,
     "start_time": "2025-03-12T14:55:18.236193",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def split_data(tokenized_data):\n",
    "    # split by class first\n",
    "    class1 = tokenized_data[tokenized_data[\"Verdict\"] == -1]\n",
    "    class2 = tokenized_data[tokenized_data[\"Verdict\"] == 1]\n",
    "    class3 = tokenized_data[tokenized_data[\"Verdict\"] == 0]\n",
    "    \n",
    "    # split tokenized data into 80-20 split before engineering features\n",
    "    count80 = math.floor(class1.shape[0] * 0.8)\n",
    "    count20 = class1.shape[0] - count80\n",
    "    sample1a = class1.sample(count80)\n",
    "    sample1b = class1.drop(sample1a.index)\n",
    "    sample2a = class2.sample(count80)\n",
    "    sample2b = class2.drop(sample2a.index)\n",
    "    sample3a = class3.sample(count80)\n",
    "    sample3b = class3.drop(sample3a.index)\n",
    "    tokenized_train = pd.concat((sample1a, sample2a, sample3a), axis=0)\n",
    "    tokenized_test = pd.concat((sample1b, sample2b, sample3b), axis=0)\n",
    "    \n",
    "    y_train = tokenized_train[\"Verdict\"]\n",
    "    y_test = tokenized_test[\"Verdict\"]\n",
    "    return tokenized_train, tokenized_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab15563",
   "metadata": {
    "papermill": {
     "duration": 0.006407,
     "end_time": "2025-03-12T14:55:18.267847",
     "exception": false,
     "start_time": "2025-03-12T14:55:18.261440",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Feature Engineering\n",
    "After processing the Text into tokens, we have to derive features from the tokens. A few approaches available:\n",
    "- Bag-of-Words representation\n",
    "- Document term matrix with tf-idf weights\n",
    "- PPMI term context matrix (?)\n",
    "- Dense word embedding (Word2Vec)\n",
    "- Can also apply PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f3180d28",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-12T14:55:18.282946Z",
     "iopub.status.busy": "2025-03-12T14:55:18.282546Z",
     "iopub.status.idle": "2025-03-12T14:55:18.292713Z",
     "shell.execute_reply": "2025-03-12T14:55:18.291222Z"
    },
    "papermill": {
     "duration": 0.020208,
     "end_time": "2025-03-12T14:55:18.294844",
     "exception": false,
     "start_time": "2025-03-12T14:55:18.274636",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# obtain an embedding vector representing each sentence by taking sum over all word embeddings in each sentence\n",
    "def compute_embeddings(corpus):\n",
    "    doc_embeddings = []\n",
    "    for doc in corpus:\n",
    "        doc_embedding = np.zeros(word_embedding_dim, dtype=np.float64)\n",
    "        for token in doc:\n",
    "            if token in glove_word_embeddings:\n",
    "                doc_embedding = np.add(doc_embedding, glove_word_embeddings[token])\n",
    "        doc_embeddings.append(doc_embedding)\n",
    "    return np.array(doc_embeddings)\n",
    "\n",
    "def compute_tfidf(tokens, vectorizer):\n",
    "    try:\n",
    "        vectorizer.transform(tokens)\n",
    "    except NotFittedError as e:\n",
    "        print(\"fitting vectorizer!\")\n",
    "        vectorizer.fit(tokens)\n",
    "\n",
    "    return vectorizer.transform(tokens).toarray()\n",
    "\n",
    "# compute counts of certain words\n",
    "def get_additional_features(corpus):\n",
    "    result = []\n",
    "    for doc in corpus:\n",
    "        counts = [\n",
    "            count_ref_other_people(doc),\n",
    "            count_ref_self(doc),\n",
    "            count_causal(doc),\n",
    "            count_negation(doc),\n",
    "            count_promise(doc),\n",
    "            count_all_or_nothing(doc),\n",
    "            count_prosocial(doc),\n",
    "            count_together(doc),\n",
    "        ]\n",
    "        result.append(counts)\n",
    "    return np.array(result)\n",
    "\n",
    "# some counts need lemmatized vocabulary\n",
    "def get_additional_features2(lemmas):\n",
    "    result = []\n",
    "    for doc in lemmas:\n",
    "        counts = [\n",
    "            count_accusation(doc)\n",
    "        ]\n",
    "        result.append(counts)\n",
    "    return np.array(result)\n",
    "\n",
    "def conduct_pca(features, pca):\n",
    "    try:\n",
    "        pca.transform(features)\n",
    "    except NotFittedError as e:\n",
    "        print(\"fitting pca!\")\n",
    "        pca.fit(features)\n",
    "        print(\"PCA cumulative variance: \", np.cumsum(pca.explained_variance_ratio_))\n",
    "    return pca.transform(features)\n",
    "    \n",
    "\n",
    "# pipeline which outputs all features, run with train data first\n",
    "def get_features(tokenized_data, model):\n",
    "    # sparse = [\n",
    "    #     compute_tfidf(tokenized_data[\"Pos\"], model.pos_vectorizer),\n",
    "    #     get_additional_features(tokenized_data[\"Tokens\"]),\n",
    "    #     get_additional_features2(tokenized_data[\"Content\"])\n",
    "    # ]\n",
    "    # sparse = np.concatenate(sparse, axis=1)\n",
    "    # sparse_pca = conduct_pca(sparse, model.pca)\n",
    "    features = [\n",
    "        compute_embeddings(tokenized_data[\"Tokens\"]),\n",
    "        # sparse_pca\n",
    "    ]\n",
    "    features = np.concatenate(features, axis=1)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce5cfa0",
   "metadata": {
    "papermill": {
     "duration": 0.007543,
     "end_time": "2025-03-12T14:55:18.309321",
     "exception": false,
     "start_time": "2025-03-12T14:55:18.301778",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Modelling\n",
    "For the model, we can choose from these 3 approaches:\n",
    "- Naive Bayes (generative classifier)\n",
    "- Logistic Regression (discriminative classifier)\n",
    "- Multi-Layer Perceptron Neural Network (discriminative classifier)\n",
    "\n",
    "To obtain a baseline model, we will only do this for now:\n",
    "- Features: Bag-of-Words, one-hot encoding of documents\n",
    "- Model: Naive Bayes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "05d9a464",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-12T14:55:18.323167Z",
     "iopub.status.busy": "2025-03-12T14:55:18.322735Z",
     "iopub.status.idle": "2025-03-12T14:55:18.329519Z",
     "shell.execute_reply": "2025-03-12T14:55:18.328269Z"
    },
    "papermill": {
     "duration": 0.016091,
     "end_time": "2025-03-12T14:55:18.331614",
     "exception": false,
     "start_time": "2025-03-12T14:55:18.315523",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Neural Network works on word embeddings of sentence and other features\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.pos_vectorizer = TfidfVectorizer(ngram_range=(1, 1), lowercase=False, tokenizer=lambda x:x, token_pattern=None)\n",
    "        self.pca = PCA(n_components=15)\n",
    "\n",
    "        # define multilayer perceptron layers\n",
    "        self.fc1 = nn.Linear(300, 64)\n",
    "        self.fc2 = nn.Linear(64, 3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c39114c",
   "metadata": {
    "papermill": {
     "duration": 0.005969,
     "end_time": "2025-03-12T14:55:18.343952",
     "exception": false,
     "start_time": "2025-03-12T14:55:18.337983",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Results\n",
    "Predict results and compute performance of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ca2b41f8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-12T14:55:18.358998Z",
     "iopub.status.busy": "2025-03-12T14:55:18.358567Z",
     "iopub.status.idle": "2025-03-12T14:55:18.368155Z",
     "shell.execute_reply": "2025-03-12T14:55:18.366130Z"
    },
    "papermill": {
     "duration": 0.020412,
     "end_time": "2025-03-12T14:55:18.370568",
     "exception": false,
     "start_time": "2025-03-12T14:55:18.350156",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_performance_per_class(model, X_test, y_test):\n",
    "    # y_pred = model.predict(X_test)\n",
    "    pred_model = model(X_test_tensor)\n",
    "    _, y_pred = pred_model.max(1)\n",
    "    # need to convert values of 2 in y_pred back to -1\n",
    "    y_pred = y_pred.numpy()\n",
    "    y_test = y_test.numpy()\n",
    "    y_pred[y_pred == 2] = -1\n",
    "    y_test[y_test == 2] = -1\n",
    "    print(y_pred)\n",
    "    print(y_pred.shape)\n",
    "    print(y_test.shape)\n",
    "    # compute separately for each class\n",
    "    result = []\n",
    "    for c in [-1, 0, 1]:\n",
    "        TP = np.sum((y_pred == c) & (y_test == c))\n",
    "        FP = np.sum((y_pred == c) & (y_test != c))\n",
    "        FN = np.sum((y_pred != c) & (y_test == c))\n",
    "        TN = np.sum((y_pred != c) & (y_test != c))\n",
    "        precision = TP / (TP + FP)\n",
    "        recall = TP / (TP + FN)\n",
    "        F1 = 2 * (precision * recall) / (precision + recall)\n",
    "        result.append([c, precision, recall, F1])\n",
    "    return pd.DataFrame(data=np.array(result), columns=[\"Class\", \"Precision\", \"Recall\", \"F1\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4a0befc5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-12T14:55:18.388435Z",
     "iopub.status.busy": "2025-03-12T14:55:18.387950Z",
     "iopub.status.idle": "2025-03-12T14:55:18.393757Z",
     "shell.execute_reply": "2025-03-12T14:55:18.392128Z"
    },
    "papermill": {
     "duration": 0.018203,
     "end_time": "2025-03-12T14:55:18.396595",
     "exception": false,
     "start_time": "2025-03-12T14:55:18.378392",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_macro_f1(f1_scores):\n",
    "    return np.mean(f1_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419866b0",
   "metadata": {
    "papermill": {
     "duration": 0.00671,
     "end_time": "2025-03-12T14:55:18.412115",
     "exception": false,
     "start_time": "2025-03-12T14:55:18.405405",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Run Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "db804991",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-12T14:55:18.429000Z",
     "iopub.status.busy": "2025-03-12T14:55:18.428466Z",
     "iopub.status.idle": "2025-03-12T14:55:42.589511Z",
     "shell.execute_reply": "2025-03-12T14:55:42.588241Z"
    },
    "papermill": {
     "duration": 24.171193,
     "end_time": "2025-03-12T14:55:42.591389",
     "exception": false,
     "start_time": "2025-03-12T14:55:18.420196",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows with null Sentence_id:  0\n",
      "Rows with null Text:  0\n",
      "Rows with null Verdict:  0\n",
      "Old counts:\n",
      "          Sentence_id   Text\n",
      "Verdict                    \n",
      "-1             14542  14542\n",
      " 0              2388   2388\n",
      " 1              5386   5386\n",
      "New counts:\n",
      "          Sentence_id  Text\n",
      "Verdict                   \n",
      "-1              2388  2388\n",
      " 0              2388  2388\n",
      " 1              2388  2388\n",
      "Tokenized data columns:  Index(['Sentence_id', 'Text', 'Verdict', 'Tokens', 'Pos', 'Remove', 'Content'], dtype='object')\n",
      "X_train:  (5730, 300)\n"
     ]
    }
   ],
   "source": [
    "# clean data\n",
    "cleaned_data = clean_data(train_data)\n",
    "# balanced data\n",
    "balanced_data = balance_classes(cleaned_data)\n",
    "# tokenize data\n",
    "tokenized_data = tokenize(balanced_data)\n",
    "print(\"Tokenized data columns: \", tokenized_data.columns)\n",
    "# split data\n",
    "tokenized_train, tokenized_test, y_train, y_test = split_data(tokenized_data)\n",
    "# engineer features\n",
    "model = Model()\n",
    "X_train = get_features(tokenized_train, model)\n",
    "print(\"X_train: \", X_train.shape)\n",
    "X_test = get_features(tokenized_test, model)\n",
    "\n",
    "# convert data to tensors\n",
    "y_train = y_train.replace(to_replace=-1, value=2)\n",
    "y_test = y_test.replace(to_replace=-1, value=2)\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train.to_numpy(), dtype=torch.long)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test.to_numpy(), dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a73e43ae",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-12T14:55:42.605327Z",
     "iopub.status.busy": "2025-03-12T14:55:42.604970Z",
     "iopub.status.idle": "2025-03-12T14:57:12.504730Z",
     "shell.execute_reply": "2025-03-12T14:57:12.503583Z"
    },
    "papermill": {
     "duration": 89.90918,
     "end_time": "2025-03-12T14:57:12.507014",
     "exception": false,
     "start_time": "2025-03-12T14:55:42.597834",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.4269, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.8693, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.8346, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.8149, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.8005, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7888, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7790, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7703, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7626, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7557, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# train neural network\n",
    "\n",
    "# define model parameters\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
    "\n",
    "num_epochs = 10000\n",
    "for n in range(num_epochs):\n",
    "    y_pred = model(X_train_tensor)\n",
    "    loss = loss_fn(y_pred, y_train_tensor)\n",
    "    if n % 1000 == 0:\n",
    "        print(loss)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "844a7dd5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-12T14:57:12.525490Z",
     "iopub.status.busy": "2025-03-12T14:57:12.524795Z",
     "iopub.status.idle": "2025-03-12T14:57:12.540026Z",
     "shell.execute_reply": "2025-03-12T14:57:12.539003Z"
    },
    "papermill": {
     "duration": 0.025116,
     "end_time": "2025-03-12T14:57:12.541766",
     "exception": false,
     "start_time": "2025-03-12T14:57:12.516650",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1 -1 -1 ...  0  0  0]\n",
      "(1434,)\n",
      "(1434,)\n",
      "   Class  Precision    Recall        F1\n",
      "0   -1.0   0.677019  0.684100  0.680541\n",
      "1    0.0   0.602911  0.606695  0.604797\n",
      "2    1.0   0.648936  0.638075  0.643460\n",
      "Macro F1:  0.6429325606067761\n"
     ]
    }
   ],
   "source": [
    "# compute results\n",
    "results = compute_performance_per_class(model, X_test_tensor, y_test_tensor)\n",
    "print(results)\n",
    "macro_f1 = compute_macro_f1(results['F1'])\n",
    "print(\"Macro F1: \", macro_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c66db414",
   "metadata": {
    "papermill": {
     "duration": 0.006367,
     "end_time": "2025-03-12T14:57:12.556259",
     "exception": false,
     "start_time": "2025-03-12T14:57:12.549892",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4226311f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-12T14:57:12.570529Z",
     "iopub.status.busy": "2025-03-12T14:57:12.570159Z",
     "iopub.status.idle": "2025-03-12T14:57:12.576278Z",
     "shell.execute_reply": "2025-03-12T14:57:12.575292Z"
    },
    "papermill": {
     "duration": 0.015131,
     "end_time": "2025-03-12T14:57:12.577850",
     "exception": false,
     "start_time": "2025-03-12T14:57:12.562719",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_result(test, y_pred, filename):\n",
    "    # clean data\n",
    "    # cleaned_data = clean_data(test)\n",
    "    # balanced data\n",
    "    # balanced_data = balance_classes(cleaned_data)\n",
    "    # tokenize data\n",
    "    print(np.array(test).shape)\n",
    "    tokenized_data = tokenize(test)\n",
    "    print(\"Tokenized data columns: \", tokenized_data.columns)\n",
    "    X_test = get_features(tokenized_data, model)\n",
    "    X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "\n",
    "    pred_model = model(X_test_tensor)\n",
    "    _, y_pred = pred_model.max(1)\n",
    "    y_pred = y_pred.numpy()\n",
    "    y_pred[y_pred == 2] = -1\n",
    "    \n",
    "    ''' generate csv file base on the y_pred '''\n",
    "    test['Verdict'] = pd.Series(y_pred)\n",
    "    test.drop(columns=['Text'], inplace=True)\n",
    "    test.to_csv(filename, index=False)\n",
    "\n",
    "# output_filename = f\"A2_{_NAME}_{_STUDENT_NUM}.csv\"\n",
    "# generate_result(test_data, y_pred, output_filename)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 11087755,
     "sourceId": 93118,
     "sourceType": "competition"
    },
    {
     "datasetId": 6847560,
     "sourceId": 10999989,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30918,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 163.729165,
   "end_time": "2025-03-12T14:57:15.378450",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-03-12T14:54:31.649285",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
