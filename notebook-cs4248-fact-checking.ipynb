{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c79a45b7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-12T15:22:00.402742Z",
     "iopub.status.busy": "2025-03-12T15:22:00.402396Z",
     "iopub.status.idle": "2025-03-12T15:22:00.408593Z",
     "shell.execute_reply": "2025-03-12T15:22:00.407349Z"
    },
    "papermill": {
     "duration": 0.01554,
     "end_time": "2025-03-12T15:22:00.410324",
     "exception": false,
     "start_time": "2025-03-12T15:22:00.394784",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TODO: Replace with your Student NET ID\n",
    "_NAME = \"Jason Lee Jia Xuan\"\n",
    "_STUDENT_NUM = 'E0957670'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e4e16f8",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-03-12T15:22:00.423331Z",
     "iopub.status.busy": "2025-03-12T15:22:00.422812Z",
     "iopub.status.idle": "2025-03-12T15:22:12.295185Z",
     "shell.execute_reply": "2025-03-12T15:22:12.293753Z"
    },
    "papermill": {
     "duration": 11.881411,
     "end_time": "2025-03-12T15:22:12.297494",
     "exception": false,
     "start_time": "2025-03-12T15:22:00.416083",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spaCy pipeline:  ['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']\n",
      "/kaggle/input/cs-4248-fact-checking-2420/train.csv\n",
      "/kaggle/input/cs-4248-fact-checking-2420/test.csv\n",
      "/kaggle/input/glove-6b/glove.6B.200d.txt\n",
      "/kaggle/input/glove-6b/glove.6B.50d.txt\n",
      "/kaggle/input/glove-6b/glove.6B.300d.txt\n",
      "/kaggle/input/glove-6b/glove.6B.100d.txt\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import math\n",
    "import time\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from sklearn.metrics import f1_score\n",
    "# for tokenizing and extracting bag-of-words vectors\n",
    "# from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.exceptions import NotFittedError\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# tokenizer\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "print(\"spaCy pipeline: \", nlp.pipe_names)\n",
    "\n",
    "# multilayer perceptron\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d04457",
   "metadata": {
    "papermill": {
     "duration": 0.005454,
     "end_time": "2025-03-12T15:22:12.308994",
     "exception": false,
     "start_time": "2025-03-12T15:22:12.303540",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d3e89a9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-12T15:22:12.321398Z",
     "iopub.status.busy": "2025-03-12T15:22:12.320778Z",
     "iopub.status.idle": "2025-03-12T15:22:12.439704Z",
     "shell.execute_reply": "2025-03-12T15:22:12.438668Z"
    },
    "papermill": {
     "duration": 0.127086,
     "end_time": "2025-03-12T15:22:12.441555",
     "exception": false,
     "start_time": "2025-03-12T15:22:12.314469",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence_id</th>\n",
       "      <th>Text</th>\n",
       "      <th>Verdict</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>I think we've seen a deterioration of values.</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>I think for a while as a nation we condoned th...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>For a while, as I recall, it even seems to me ...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>So we've seen a deterioration in values, and o...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>We got away, we got into this feeling that val...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Sentence_id                                               Text  Verdict\n",
       "0            1      I think we've seen a deterioration of values.       -1\n",
       "1            2  I think for a while as a nation we condoned th...       -1\n",
       "2            3  For a while, as I recall, it even seems to me ...       -1\n",
       "3            4  So we've seen a deterioration in values, and o...       -1\n",
       "4            5  We got away, we got into this feeling that val...       -1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import data\n",
    "train_data = pd.read_csv(\"../input/cs-4248-fact-checking-2420/train.csv\")\n",
    "test_data = pd.read_csv(\"../input/cs-4248-fact-checking-2420/test.csv\")\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1682a37",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-12T15:22:12.454837Z",
     "iopub.status.busy": "2025-03-12T15:22:12.454507Z",
     "iopub.status.idle": "2025-03-12T15:22:46.540529Z",
     "shell.execute_reply": "2025-03-12T15:22:46.539278Z"
    },
    "papermill": {
     "duration": 34.098805,
     "end_time": "2025-03-12T15:22:46.546589",
     "exception": false,
     "start_time": "2025-03-12T15:22:12.447784",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400000 words loaded!\n",
      "time taken: 34.07804799079895\n"
     ]
    }
   ],
   "source": [
    "# import GloVe word embeddings\n",
    "glove_word_embeddings = {}\n",
    "word_embedding_dim = 300 # adjust as necessary\n",
    "with open(\"/kaggle/input/glove-6b/glove.6B.300d.txt\", 'r', encoding=\"utf-8\") as file:\n",
    "    start = time.time()\n",
    "    for line in file:\n",
    "        spl = line.split()\n",
    "        word = spl[0]\n",
    "        embedding = spl[1:]\n",
    "        glove_word_embeddings[word] = np.array(embedding, dtype=np.float64)\n",
    "    end = time.time()\n",
    "    print(f\"{len(glove_word_embeddings)} words loaded!\")\n",
    "    print(f\"time taken: {end - start}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c16e2e",
   "metadata": {
    "papermill": {
     "duration": 0.005562,
     "end_time": "2025-03-12T15:22:46.558094",
     "exception": false,
     "start_time": "2025-03-12T15:22:46.552532",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Data Preprocessing\n",
    "Do some data preprocessing so that the data is of a good quality\n",
    "- Clean data\n",
    "- Resolve imbalances\n",
    "    - Sampling\n",
    "    - Data augmentation (?)\n",
    "- Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3585636e",
   "metadata": {
    "papermill": {
     "duration": 0.005437,
     "end_time": "2025-03-12T15:22:46.569339",
     "exception": false,
     "start_time": "2025-03-12T15:22:46.563902",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Clean Data\n",
    "Obtain a standardized set of data\n",
    "- Data should not contain missing values\n",
    "- Data should not have duplicates. If there are any duplicates, remove them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8ced935a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-12T15:22:46.582651Z",
     "iopub.status.busy": "2025-03-12T15:22:46.582226Z",
     "iopub.status.idle": "2025-03-12T15:22:46.587600Z",
     "shell.execute_reply": "2025-03-12T15:22:46.586525Z"
    },
    "papermill": {
     "duration": 0.014336,
     "end_time": "2025-03-12T15:22:46.589503",
     "exception": false,
     "start_time": "2025-03-12T15:22:46.575167",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# remove missing values and remove duplicates\n",
    "def clean_data(data):\n",
    "    # count missing data, I think kaggle tells us the data does not have missing values\n",
    "    print(\"Rows with null Sentence_id: \", sum(data[\"Sentence_id\"].isnull()))\n",
    "    print(\"Rows with null Text: \", sum(data[\"Text\"].isnull()))\n",
    "    print(\"Rows with null Verdict: \", sum(data[\"Verdict\"].isnull()))\n",
    "\n",
    "    # remove duplicates from the data\n",
    "    # set keep=False because we have no idea which label is actually correct\n",
    "    data_cleaned = data.drop_duplicates([\"Text\"], keep=False)\n",
    "    return data_cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdbb0031",
   "metadata": {
    "papermill": {
     "duration": 0.005351,
     "end_time": "2025-03-12T15:22:46.600689",
     "exception": false,
     "start_time": "2025-03-12T15:22:46.595338",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Resolve Class Imbalance\n",
    "In order to train the model properly, we need to resolve the class imbalance.\n",
    "We can either upsample or downsample.\n",
    "- For simplicity, we try downsampling here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b1e7aaa1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-12T15:22:46.613372Z",
     "iopub.status.busy": "2025-03-12T15:22:46.612981Z",
     "iopub.status.idle": "2025-03-12T15:22:46.618956Z",
     "shell.execute_reply": "2025-03-12T15:22:46.617825Z"
    },
    "papermill": {
     "duration": 0.014235,
     "end_time": "2025-03-12T15:22:46.620650",
     "exception": false,
     "start_time": "2025-03-12T15:22:46.606415",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def balance_classes(data):\n",
    "    # show how many data points there are for each verdict in the training data\n",
    "    print(\"Old counts:\\n\", data.groupby(\"Verdict\").count())\n",
    "    # obtain number of samples for smallest class\n",
    "    min_count = data.groupby(\"Verdict\").count()['Text'].min()\n",
    "    # sample from all classes this amount\n",
    "    class1 = data[data['Verdict'] == -1].sample(min_count)\n",
    "    class2 = data[data['Verdict'] == 0].sample(min_count)\n",
    "    class3 = data[data['Verdict'] == 1].sample(min_count)\n",
    "    # combine\n",
    "    data_balanced = pd.concat([class1, class2, class3], ignore_index=True)\n",
    "    # verify counts\n",
    "    print(\"New counts:\\n\", data_balanced.groupby(\"Verdict\").count())\n",
    "    return data_balanced"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e705055",
   "metadata": {
    "papermill": {
     "duration": 0.005512,
     "end_time": "2025-03-12T15:22:46.631981",
     "exception": false,
     "start_time": "2025-03-12T15:22:46.626469",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Tokenization, Case Folding, Stopword and Punctuation Removal\n",
    "Perform tokenization on text data:\n",
    "- make lowercase\n",
    "- remove stopwords\n",
    "- remove punctuation\n",
    "- possible to lemmatize but it is not done here.\n",
    "\n",
    "If any sentences only contain stopwords, then remove the whole row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3a275157",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-12T15:22:46.645137Z",
     "iopub.status.busy": "2025-03-12T15:22:46.644719Z",
     "iopub.status.idle": "2025-03-12T15:22:46.652239Z",
     "shell.execute_reply": "2025-03-12T15:22:46.651212Z"
    },
    "papermill": {
     "duration": 0.016366,
     "end_time": "2025-03-12T15:22:46.654258",
     "exception": false,
     "start_time": "2025-03-12T15:22:46.637892",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tokenize(data):\n",
    "    result = data.copy()\n",
    "    text = result[\"Text\"]\n",
    "    tokens = []\n",
    "    pos = []\n",
    "    remove = [] # if no tokens are generated, remove it later\n",
    "    content = []\n",
    "    reject_pos = set(['DET', 'NUM', 'SYM', 'INTJ', 'X'])\n",
    "    for doc in nlp.pipe(text, batch_size=50):\n",
    "        # remove tokens that we don't want\n",
    "        doc_clean = [token for token in doc if not token.is_punct and token.pos_ not in reject_pos]\n",
    "        # tokenize\n",
    "        t = np.array([token.lower_ for token in doc_clean])\n",
    "        p = np.array([token.pos_ for token in doc_clean])\n",
    "        x = np.array([token.lemma_.lower() for token in doc_clean if not token.is_stop]) # lemmatized, stopword removed token list\n",
    "        remove.append(t.shape[0] == 0)\n",
    "        tokens.append(t)\n",
    "        content.append(x)\n",
    "        pos.append(p)\n",
    "\n",
    "    result[\"Tokens\"] = tokens\n",
    "    result[\"Pos\"] = pos\n",
    "    result[\"Remove\"] = remove\n",
    "    result[\"Content\"] = content\n",
    "    result = result.drop(result[result[\"Remove\"]].index)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "334dd0fe",
   "metadata": {
    "papermill": {
     "duration": 0.005482,
     "end_time": "2025-03-12T15:22:46.665582",
     "exception": false,
     "start_time": "2025-03-12T15:22:46.660100",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Data Split\n",
    "Split data into training, validation, and test sets for training a model.\n",
    "We will use a 80-10-10 split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "99a3261d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-12T15:22:46.678564Z",
     "iopub.status.busy": "2025-03-12T15:22:46.678119Z",
     "iopub.status.idle": "2025-03-12T15:22:46.685234Z",
     "shell.execute_reply": "2025-03-12T15:22:46.683880Z"
    },
    "papermill": {
     "duration": 0.015688,
     "end_time": "2025-03-12T15:22:46.687086",
     "exception": false,
     "start_time": "2025-03-12T15:22:46.671398",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def split_data(tokenized_data):\n",
    "    # split by class first\n",
    "    class1 = tokenized_data[tokenized_data[\"Verdict\"] == -1]\n",
    "    class2 = tokenized_data[tokenized_data[\"Verdict\"] == 1]\n",
    "    class3 = tokenized_data[tokenized_data[\"Verdict\"] == 0]\n",
    "    \n",
    "    # split tokenized data into 80-20 split before engineering features\n",
    "    count80 = math.floor(class1.shape[0] * 0.8)\n",
    "    count20 = class1.shape[0] - count80\n",
    "    sample1a = class1.sample(count80)\n",
    "    sample1b = class1.drop(sample1a.index)\n",
    "    sample2a = class2.sample(count80)\n",
    "    sample2b = class2.drop(sample2a.index)\n",
    "    sample3a = class3.sample(count80)\n",
    "    sample3b = class3.drop(sample3a.index)\n",
    "    tokenized_train = pd.concat((sample1a, sample2a, sample3a), axis=0)\n",
    "    tokenized_test = pd.concat((sample1b, sample2b, sample3b), axis=0)\n",
    "    \n",
    "    y_train = tokenized_train[\"Verdict\"]\n",
    "    y_test = tokenized_test[\"Verdict\"]\n",
    "    return tokenized_train, tokenized_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c079e7d4",
   "metadata": {
    "papermill": {
     "duration": 0.00607,
     "end_time": "2025-03-12T15:22:46.699195",
     "exception": false,
     "start_time": "2025-03-12T15:22:46.693125",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Feature Engineering\n",
    "After processing the Text into tokens, we have to derive features from the tokens. A few approaches available:\n",
    "- Bag-of-Words representation\n",
    "- Document term matrix with tf-idf weights\n",
    "- PPMI term context matrix (?)\n",
    "- Dense word embedding (Word2Vec)\n",
    "- Can also apply PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "41b0aaae",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-12T15:22:46.712622Z",
     "iopub.status.busy": "2025-03-12T15:22:46.712261Z",
     "iopub.status.idle": "2025-03-12T15:22:46.722712Z",
     "shell.execute_reply": "2025-03-12T15:22:46.721541Z"
    },
    "papermill": {
     "duration": 0.019505,
     "end_time": "2025-03-12T15:22:46.724650",
     "exception": false,
     "start_time": "2025-03-12T15:22:46.705145",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def count(doc, words):\n",
    "    count = 0\n",
    "    for token in doc:\n",
    "        if token in words:\n",
    "            count += 1\n",
    "    return count\n",
    "\n",
    "def count_ref_other_people(doc):\n",
    "    words = set([\"you\", \"others\", \"people\", \"yourself\", \"yourselves\", \"your\", \"\"])\n",
    "    return count(doc, words)\n",
    "\n",
    "def count_ref_self(doc):\n",
    "    words = set([\"i\", \"self\", \"me\", \"myself\", \"mine\", \"friends\", \"friend\", \"family\", \"buddy\", \"mate\"])\n",
    "    return count(doc, words)\n",
    "\n",
    "def count_causal(doc):\n",
    "    words = set([\"cause\", \"because\", \"effect\", \"hence\", \"therefore\", \"thus\", \"since\", \"reason\", \"due\"])\n",
    "    return count(doc, words)\n",
    "\n",
    "def count_negation(doc):\n",
    "    words = set([\"no\", \"not\", \"neither\", \"none\", \"nobody\", \"nothing\", \"nowhere\", \"hardly\", \"seldom\", \"little\"])\n",
    "    return count(doc, words)\n",
    "\n",
    "def count_promise(doc):\n",
    "    words = set([\"\\'ll\", \"will\", \"should\", \"future\", \"believe\", \"think\", \"consider\", \"propose\", \"want\", \"suspect\", \"suppose\", \"time\", \"come\", \"upcoming\"])\n",
    "    return count(doc, words)\n",
    "\n",
    "def count_all_or_nothing(doc):\n",
    "    words = set([\"everything\", \"nothing\", \"everyone\", \"all\", \"no\", \"never\", \"always\", ])\n",
    "    return count(doc, words)\n",
    "\n",
    "def count_prosocial(doc):\n",
    "    words = set([\"care\", \"help\", \"please\", \"thank\", \"thanks\", \"support\", \"trust\", \"faith\"])\n",
    "    return count(doc, words)\n",
    "\n",
    "def count_together(doc):\n",
    "    words = set([\"we\", \"us\", \"our\", \"ours\", \"together\"])\n",
    "    return count(doc, words)\n",
    "\n",
    "def count_accusation(doc):\n",
    "    words = set([\"lie\", \"steal\", \"cheat\", \"betray\", \"deceive\", \"manipulate\", \"harmed\", \"ruin\", \"destroy\", \"fake\", \"liar\", \"cheater\", \"fraud\", \"backstabber\", \"traitor\", \"thief\", \"hypocrite\", \"coward\", \"fool\", \"idiot\", \"moron\"])\n",
    "    return count(doc, words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5118821f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-12T15:22:46.738272Z",
     "iopub.status.busy": "2025-03-12T15:22:46.737814Z",
     "iopub.status.idle": "2025-03-12T15:22:46.748076Z",
     "shell.execute_reply": "2025-03-12T15:22:46.746857Z"
    },
    "papermill": {
     "duration": 0.019633,
     "end_time": "2025-03-12T15:22:46.750227",
     "exception": false,
     "start_time": "2025-03-12T15:22:46.730594",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# obtain an embedding vector representing each sentence by taking sum over all word embeddings in each sentence\n",
    "def compute_embeddings(corpus):\n",
    "    doc_embeddings = []\n",
    "    for doc in corpus:\n",
    "        doc_embedding = np.zeros(word_embedding_dim, dtype=np.float64)\n",
    "        for token in doc:\n",
    "            if token in glove_word_embeddings:\n",
    "                doc_embedding = np.add(doc_embedding, glove_word_embeddings[token])\n",
    "        doc_embeddings.append(doc_embedding)\n",
    "    return np.array(doc_embeddings)\n",
    "\n",
    "def compute_tfidf(tokens, vectorizer):\n",
    "    try:\n",
    "        vectorizer.transform(tokens)\n",
    "    except NotFittedError as e:\n",
    "        print(\"fitting vectorizer!\")\n",
    "        vectorizer.fit(tokens)\n",
    "\n",
    "    return vectorizer.transform(tokens).toarray()\n",
    "\n",
    "# compute counts of certain words\n",
    "def get_additional_features(corpus):\n",
    "    result = []\n",
    "    for doc in corpus:\n",
    "        counts = [\n",
    "            count_ref_other_people(doc),\n",
    "            count_ref_self(doc),\n",
    "            count_causal(doc),\n",
    "            count_negation(doc),\n",
    "            count_promise(doc),\n",
    "            count_all_or_nothing(doc),\n",
    "            count_prosocial(doc),\n",
    "            count_together(doc),\n",
    "        ]\n",
    "        result.append(counts)\n",
    "    return np.array(result)\n",
    "\n",
    "# some counts need lemmatized vocabulary\n",
    "def get_additional_features2(lemmas):\n",
    "    result = []\n",
    "    for doc in lemmas:\n",
    "        counts = [\n",
    "            count_accusation(doc)\n",
    "        ]\n",
    "        result.append(counts)\n",
    "    return np.array(result)\n",
    "\n",
    "def conduct_pca(features, pca):\n",
    "    try:\n",
    "        pca.transform(features)\n",
    "    except NotFittedError as e:\n",
    "        print(\"fitting pca!\")\n",
    "        pca.fit(features)\n",
    "        print(\"PCA cumulative variance: \", np.cumsum(pca.explained_variance_ratio_))\n",
    "    return pca.transform(features)\n",
    "    \n",
    "\n",
    "# pipeline which outputs all features, run with train data first\n",
    "def get_features(tokenized_data, model):\n",
    "    sparse = [\n",
    "        compute_tfidf(tokenized_data[\"Pos\"], model.pos_vectorizer),\n",
    "        get_additional_features(tokenized_data[\"Tokens\"]),\n",
    "        get_additional_features2(tokenized_data[\"Content\"])\n",
    "    ]\n",
    "    sparse = np.concatenate(sparse, axis=1)\n",
    "    sparse_pca = conduct_pca(sparse, model.pca)\n",
    "    features = [\n",
    "        compute_embeddings(tokenized_data[\"Tokens\"]),\n",
    "        sparse_pca\n",
    "    ]\n",
    "    features = np.concatenate(features, axis=1)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4421bc83",
   "metadata": {
    "papermill": {
     "duration": 0.005511,
     "end_time": "2025-03-12T15:22:46.762800",
     "exception": false,
     "start_time": "2025-03-12T15:22:46.757289",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Modelling\n",
    "For the model, we can choose from these 3 approaches:\n",
    "- Naive Bayes (generative classifier)\n",
    "- Logistic Regression (discriminative classifier)\n",
    "- Multi-Layer Perceptron Neural Network (discriminative classifier)\n",
    "\n",
    "To obtain a baseline model, we will only do this for now:\n",
    "- Features: Bag-of-Words, one-hot encoding of documents\n",
    "- Model: Naive Bayes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "79cc837b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-12T15:22:46.776855Z",
     "iopub.status.busy": "2025-03-12T15:22:46.776352Z",
     "iopub.status.idle": "2025-03-12T15:22:46.782711Z",
     "shell.execute_reply": "2025-03-12T15:22:46.781424Z"
    },
    "papermill": {
     "duration": 0.015711,
     "end_time": "2025-03-12T15:22:46.784647",
     "exception": false,
     "start_time": "2025-03-12T15:22:46.768936",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Neural Network works on word embeddings of sentence and other features\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.pos_vectorizer = TfidfVectorizer(ngram_range=(1, 1), lowercase=False, tokenizer=lambda x:x, token_pattern=None)\n",
    "        self.pca = PCA(n_components=15)\n",
    "\n",
    "        # define multilayer perceptron layers\n",
    "        self.fc1 = nn.Linear(315, 64)\n",
    "        self.fc2 = nn.Linear(64, 3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d233a02",
   "metadata": {
    "papermill": {
     "duration": 0.005825,
     "end_time": "2025-03-12T15:22:46.796533",
     "exception": false,
     "start_time": "2025-03-12T15:22:46.790708",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Results\n",
    "Predict results and compute performance of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a0876765",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-12T15:22:46.810151Z",
     "iopub.status.busy": "2025-03-12T15:22:46.809726Z",
     "iopub.status.idle": "2025-03-12T15:22:46.817707Z",
     "shell.execute_reply": "2025-03-12T15:22:46.816563Z"
    },
    "papermill": {
     "duration": 0.016881,
     "end_time": "2025-03-12T15:22:46.819505",
     "exception": false,
     "start_time": "2025-03-12T15:22:46.802624",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_performance_per_class(model, X_test, y_test):\n",
    "    # y_pred = model.predict(X_test)\n",
    "    pred_model = model(X_test_tensor)\n",
    "    _, y_pred = pred_model.max(1)\n",
    "    # need to convert values of 2 in y_pred back to -1\n",
    "    y_pred = y_pred.numpy()\n",
    "    y_test = y_test.numpy()\n",
    "    y_pred[y_pred == 2] = -1\n",
    "    y_test[y_test == 2] = -1\n",
    "    print(y_pred)\n",
    "    print(y_pred.shape)\n",
    "    print(y_test.shape)\n",
    "    # compute separately for each class\n",
    "    result = []\n",
    "    for c in [-1, 0, 1]:\n",
    "        TP = np.sum((y_pred == c) & (y_test == c))\n",
    "        FP = np.sum((y_pred == c) & (y_test != c))\n",
    "        FN = np.sum((y_pred != c) & (y_test == c))\n",
    "        TN = np.sum((y_pred != c) & (y_test != c))\n",
    "        precision = TP / (TP + FP)\n",
    "        recall = TP / (TP + FN)\n",
    "        F1 = 2 * (precision * recall) / (precision + recall)\n",
    "        result.append([c, precision, recall, F1])\n",
    "    return pd.DataFrame(data=np.array(result), columns=[\"Class\", \"Precision\", \"Recall\", \"F1\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b5f09753",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-12T15:22:46.832538Z",
     "iopub.status.busy": "2025-03-12T15:22:46.832175Z",
     "iopub.status.idle": "2025-03-12T15:22:46.836754Z",
     "shell.execute_reply": "2025-03-12T15:22:46.835565Z"
    },
    "papermill": {
     "duration": 0.018713,
     "end_time": "2025-03-12T15:22:46.844239",
     "exception": false,
     "start_time": "2025-03-12T15:22:46.825526",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_macro_f1(f1_scores):\n",
    "    return np.mean(f1_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "945ffbcc",
   "metadata": {
    "papermill": {
     "duration": 0.005468,
     "end_time": "2025-03-12T15:22:46.855681",
     "exception": false,
     "start_time": "2025-03-12T15:22:46.850213",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Run Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "98fa7f05",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-12T15:22:46.868716Z",
     "iopub.status.busy": "2025-03-12T15:22:46.868365Z",
     "iopub.status.idle": "2025-03-12T15:23:11.919453Z",
     "shell.execute_reply": "2025-03-12T15:23:11.918196Z"
    },
    "papermill": {
     "duration": 25.060218,
     "end_time": "2025-03-12T15:23:11.921831",
     "exception": false,
     "start_time": "2025-03-12T15:22:46.861613",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows with null Sentence_id:  0\n",
      "Rows with null Text:  0\n",
      "Rows with null Verdict:  0\n",
      "Old counts:\n",
      "          Sentence_id   Text\n",
      "Verdict                    \n",
      "-1             14542  14542\n",
      " 0              2388   2388\n",
      " 1              5386   5386\n",
      "New counts:\n",
      "          Sentence_id  Text\n",
      "Verdict                   \n",
      "-1              2388  2388\n",
      " 0              2388  2388\n",
      " 1              2388  2388\n",
      "Tokenized data columns:  Index(['Sentence_id', 'Text', 'Verdict', 'Tokens', 'Pos', 'Remove', 'Content'], dtype='object')\n",
      "fitting vectorizer!\n",
      "fitting pca!\n",
      "PCA cumulative variance:  [0.25950501 0.49494778 0.67325259 0.75758774 0.79882526 0.83093513\n",
      " 0.85921286 0.88344035 0.90652465 0.92623478 0.93965197 0.95136481\n",
      " 0.96176876 0.97045405 0.97894384]\n",
      "X_train:  (5730, 315)\n"
     ]
    }
   ],
   "source": [
    "# clean data\n",
    "cleaned_data = clean_data(train_data)\n",
    "# balanced data\n",
    "balanced_data = balance_classes(cleaned_data)\n",
    "# tokenize data\n",
    "tokenized_data = tokenize(balanced_data)\n",
    "print(\"Tokenized data columns: \", tokenized_data.columns)\n",
    "# split data\n",
    "tokenized_train, tokenized_test, y_train, y_test = split_data(tokenized_data)\n",
    "# engineer features\n",
    "model = Model()\n",
    "X_train = get_features(tokenized_train, model)\n",
    "print(\"X_train: \", X_train.shape)\n",
    "X_test = get_features(tokenized_test, model)\n",
    "\n",
    "# convert data to tensors\n",
    "y_train = y_train.replace(to_replace=-1, value=2)\n",
    "y_test = y_test.replace(to_replace=-1, value=2)\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train.to_numpy(), dtype=torch.long)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test.to_numpy(), dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f9a46cfa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-12T15:23:11.945594Z",
     "iopub.status.busy": "2025-03-12T15:23:11.945226Z",
     "iopub.status.idle": "2025-03-12T15:24:46.832652Z",
     "shell.execute_reply": "2025-03-12T15:24:46.831386Z"
    },
    "papermill": {
     "duration": 94.901661,
     "end_time": "2025-03-12T15:24:46.834935",
     "exception": false,
     "start_time": "2025-03-12T15:23:11.933274",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.2613, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.8821, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.8455, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.8236, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.8076, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7948, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7842, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7751, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7672, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7601, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# train neural network\n",
    "\n",
    "# define model parameters\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
    "\n",
    "num_epochs = 10000\n",
    "for n in range(num_epochs):\n",
    "    y_pred = model(X_train_tensor)\n",
    "    loss = loss_fn(y_pred, y_train_tensor)\n",
    "    if n % 1000 == 0:\n",
    "        print(loss)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a28d06f3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-12T15:24:46.853315Z",
     "iopub.status.busy": "2025-03-12T15:24:46.852578Z",
     "iopub.status.idle": "2025-03-12T15:24:46.869763Z",
     "shell.execute_reply": "2025-03-12T15:24:46.868710Z"
    },
    "papermill": {
     "duration": 0.027355,
     "end_time": "2025-03-12T15:24:46.871613",
     "exception": false,
     "start_time": "2025-03-12T15:24:46.844258",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1 -1 -1 ...  0  1  0]\n",
      "(1434,)\n",
      "(1434,)\n",
      "   Class  Precision    Recall        F1\n",
      "0   -1.0   0.663405  0.709205  0.685541\n",
      "1    0.0   0.673333  0.633891  0.653017\n",
      "2    1.0   0.680761  0.673640  0.677182\n",
      "Macro F1:  0.6719133685364298\n"
     ]
    }
   ],
   "source": [
    "# compute results\n",
    "results = compute_performance_per_class(model, X_test_tensor, y_test_tensor)\n",
    "print(results)\n",
    "macro_f1 = compute_macro_f1(results['F1'])\n",
    "print(\"Macro F1: \", macro_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c2e937",
   "metadata": {
    "papermill": {
     "duration": 0.00643,
     "end_time": "2025-03-12T15:24:46.884827",
     "exception": false,
     "start_time": "2025-03-12T15:24:46.878397",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "69a7638c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-12T15:24:46.900079Z",
     "iopub.status.busy": "2025-03-12T15:24:46.899659Z",
     "iopub.status.idle": "2025-03-12T15:24:46.906430Z",
     "shell.execute_reply": "2025-03-12T15:24:46.905292Z"
    },
    "papermill": {
     "duration": 0.01668,
     "end_time": "2025-03-12T15:24:46.908480",
     "exception": false,
     "start_time": "2025-03-12T15:24:46.891800",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_result(test, y_pred, filename):\n",
    "    # clean data\n",
    "    # cleaned_data = clean_data(test)\n",
    "    # balanced data\n",
    "    # balanced_data = balance_classes(cleaned_data)\n",
    "    # tokenize data\n",
    "    print(np.array(test).shape)\n",
    "    tokenized_data = tokenize(test)\n",
    "    print(\"Tokenized data columns: \", tokenized_data.columns)\n",
    "    X_test = get_features(tokenized_data, model)\n",
    "    X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "\n",
    "    pred_model = model(X_test_tensor)\n",
    "    _, y_pred = pred_model.max(1)\n",
    "    y_pred = y_pred.numpy()\n",
    "    y_pred[y_pred == 2] = -1\n",
    "    \n",
    "    ''' generate csv file base on the y_pred '''\n",
    "    test['Verdict'] = pd.Series(y_pred)\n",
    "    test.drop(columns=['Text'], inplace=True)\n",
    "    test.to_csv(filename, index=False)\n",
    "\n",
    "# output_filename = f\"A2_{_NAME}_{_STUDENT_NUM}.csv\"\n",
    "# generate_result(test_data, y_pred, output_filename)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 11087755,
     "sourceId": 93118,
     "sourceType": "competition"
    },
    {
     "datasetId": 6847560,
     "sourceId": 10999989,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30918,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 172.706154,
   "end_time": "2025-03-12T15:24:50.104019",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-03-12T15:21:57.397865",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
