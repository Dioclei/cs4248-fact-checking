{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a96d2ab6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-11T15:31:50.215974Z",
     "iopub.status.busy": "2025-03-11T15:31:50.215513Z",
     "iopub.status.idle": "2025-03-11T15:31:50.222383Z",
     "shell.execute_reply": "2025-03-11T15:31:50.220739Z"
    },
    "papermill": {
     "duration": 0.016144,
     "end_time": "2025-03-11T15:31:50.224555",
     "exception": false,
     "start_time": "2025-03-11T15:31:50.208411",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TODO: Replace with your Student NET ID\n",
    "_NAME = \"Jason Lee Jia Xuan\"\n",
    "_STUDENT_NUM = 'E0957670'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb1e6ea1",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-03-11T15:31:50.236941Z",
     "iopub.status.busy": "2025-03-11T15:31:50.236486Z",
     "iopub.status.idle": "2025-03-11T15:32:01.390616Z",
     "shell.execute_reply": "2025-03-11T15:32:01.389213Z"
    },
    "papermill": {
     "duration": 11.162963,
     "end_time": "2025-03-11T15:32:01.392936",
     "exception": false,
     "start_time": "2025-03-11T15:31:50.229973",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spaCy pipeline:  ['tok2vec', 'tagger', 'attribute_ruler', 'lemmatizer']\n",
      "/kaggle/input/cs-4248-fact-checking-2420/train.csv\n",
      "/kaggle/input/cs-4248-fact-checking-2420/test.csv\n",
      "/kaggle/input/glove-twitter-word-embeddings/glove.twitter.27B.200d.txt\n",
      "/kaggle/input/glove-twitter-word-embeddings/glove.twitter.27B.25d.txt\n",
      "/kaggle/input/glove-twitter-word-embeddings/glove.twitter.27B.50d.txt\n",
      "/kaggle/input/glove-twitter-word-embeddings/glove.twitter.27B.100d.txt\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import time\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from sklearn.metrics import f1_score\n",
    "# for tokenizing and extracting bag-of-words vectors\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "# from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# tokenizer\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp.select_pipes(disable=[\"parser\", \"ner\"]) # disable some components for performance\n",
    "print(\"spaCy pipeline: \", nlp.pipe_names)\n",
    "# vectorizer\n",
    "vectorizer = TfidfVectorizer(lowercase=False, tokenizer=lambda x:x, token_pattern=None)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7cd1b1a",
   "metadata": {
    "papermill": {
     "duration": 0.004922,
     "end_time": "2025-03-11T15:32:01.403177",
     "exception": false,
     "start_time": "2025-03-11T15:32:01.398255",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cdf7cdb6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-11T15:32:01.414377Z",
     "iopub.status.busy": "2025-03-11T15:32:01.413734Z",
     "iopub.status.idle": "2025-03-11T15:32:01.547333Z",
     "shell.execute_reply": "2025-03-11T15:32:01.546094Z"
    },
    "papermill": {
     "duration": 0.141218,
     "end_time": "2025-03-11T15:32:01.549207",
     "exception": false,
     "start_time": "2025-03-11T15:32:01.407989",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence_id</th>\n",
       "      <th>Text</th>\n",
       "      <th>Verdict</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>I think we've seen a deterioration of values.</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>I think for a while as a nation we condoned th...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>For a while, as I recall, it even seems to me ...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>So we've seen a deterioration in values, and o...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>We got away, we got into this feeling that val...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Sentence_id                                               Text  Verdict\n",
       "0            1      I think we've seen a deterioration of values.       -1\n",
       "1            2  I think for a while as a nation we condoned th...       -1\n",
       "2            3  For a while, as I recall, it even seems to me ...       -1\n",
       "3            4  So we've seen a deterioration in values, and o...       -1\n",
       "4            5  We got away, we got into this feeling that val...       -1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import data\n",
    "train_data = pd.read_csv(\"../input/cs-4248-fact-checking-2420/train.csv\")\n",
    "test_data = pd.read_csv(\"../input/cs-4248-fact-checking-2420/test.csv\")\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca827579",
   "metadata": {
    "papermill": {
     "duration": 0.00468,
     "end_time": "2025-03-11T15:32:01.559100",
     "exception": false,
     "start_time": "2025-03-11T15:32:01.554420",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Data Preprocessing\n",
    "Do some data preprocessing so that the data is of a good quality\n",
    "- Clean data\n",
    "- Resolve imbalances\n",
    "    - Sampling\n",
    "    - Data augmentation (?)\n",
    "- Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ce4047",
   "metadata": {
    "papermill": {
     "duration": 0.004439,
     "end_time": "2025-03-11T15:32:01.568282",
     "exception": false,
     "start_time": "2025-03-11T15:32:01.563843",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Clean Data\n",
    "Obtain a standardized set of data\n",
    "- Data should not contain missing values\n",
    "- Data should not have duplicates. If there are any duplicates, remove them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb23114e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-11T15:32:01.579817Z",
     "iopub.status.busy": "2025-03-11T15:32:01.579376Z",
     "iopub.status.idle": "2025-03-11T15:32:01.612149Z",
     "shell.execute_reply": "2025-03-11T15:32:01.610894Z"
    },
    "papermill": {
     "duration": 0.040773,
     "end_time": "2025-03-11T15:32:01.613931",
     "exception": false,
     "start_time": "2025-03-11T15:32:01.573158",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows with null Sentence_id:  0\n",
      "Rows with null Text:  0\n",
      "Rows with null Verdict:  0\n"
     ]
    }
   ],
   "source": [
    "# remove missing values and remove duplicates\n",
    "def clean_data(data):\n",
    "    # count missing data, I think kaggle tells us the data does not have missing values\n",
    "    print(\"Rows with null Sentence_id: \", sum(data[\"Sentence_id\"].isnull()))\n",
    "    print(\"Rows with null Text: \", sum(data[\"Text\"].isnull()))\n",
    "    print(\"Rows with null Verdict: \", sum(data[\"Verdict\"].isnull()))\n",
    "\n",
    "    # remove duplicates from the data\n",
    "    # set keep=False because we have no idea which label is actually correct\n",
    "    data_cleaned = data.drop_duplicates([\"Text\"], keep=False)\n",
    "    return data_cleaned\n",
    "\n",
    "train_data = clean_data(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e12b3b",
   "metadata": {
    "papermill": {
     "duration": 0.004844,
     "end_time": "2025-03-11T15:32:01.623999",
     "exception": false,
     "start_time": "2025-03-11T15:32:01.619155",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Resolve Class Imbalance\n",
    "In order to train the model properly, we need to resolve the class imbalance.\n",
    "We can either upsample or downsample.\n",
    "- For simplicity, we try downsampling here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c73c33e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-11T15:32:01.635893Z",
     "iopub.status.busy": "2025-03-11T15:32:01.635500Z",
     "iopub.status.idle": "2025-03-11T15:32:01.671355Z",
     "shell.execute_reply": "2025-03-11T15:32:01.670320Z"
    },
    "papermill": {
     "duration": 0.043923,
     "end_time": "2025-03-11T15:32:01.673062",
     "exception": false,
     "start_time": "2025-03-11T15:32:01.629139",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Old counts:\n",
      "          Sentence_id   Text\n",
      "Verdict                    \n",
      "-1             14542  14542\n",
      " 0              2388   2388\n",
      " 1              5386   5386\n",
      "New counts:\n",
      "          Sentence_id  Text\n",
      "Verdict                   \n",
      "-1              2388  2388\n",
      " 0              2388  2388\n",
      " 1              2388  2388\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence_id</th>\n",
       "      <th>Text</th>\n",
       "      <th>Verdict</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20082</td>\n",
       "      <td>As soon as she releases them, I will release.</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>17364</td>\n",
       "      <td>We have to respect one another.</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2094</td>\n",
       "      <td>If we turn to Helsinki - I'm glad you raised i...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8663</td>\n",
       "      <td>We have to be able to compete in the world mar...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>21552</td>\n",
       "      <td>And you look at our miners.</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7159</th>\n",
       "      <td>18245</td>\n",
       "      <td>That was something I concurred with...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7160</th>\n",
       "      <td>17623</td>\n",
       "      <td>We have in the Dole-Kemp economic plan, unless...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7161</th>\n",
       "      <td>16520</td>\n",
       "      <td>Many of the countries below that used to say, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7162</th>\n",
       "      <td>9886</td>\n",
       "      <td>Of course, it doesn't come out of the payroll ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7163</th>\n",
       "      <td>9127</td>\n",
       "      <td>One-third of the president's budget is at the ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7164 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Sentence_id                                               Text  Verdict\n",
       "0           20082      As soon as she releases them, I will release.       -1\n",
       "1           17364                    We have to respect one another.       -1\n",
       "2            2094  If we turn to Helsinki - I'm glad you raised i...       -1\n",
       "3            8663  We have to be able to compete in the world mar...       -1\n",
       "4           21552                        And you look at our miners.       -1\n",
       "...           ...                                                ...      ...\n",
       "7159        18245             That was something I concurred with...        1\n",
       "7160        17623  We have in the Dole-Kemp economic plan, unless...        1\n",
       "7161        16520  Many of the countries below that used to say, ...        1\n",
       "7162         9886  Of course, it doesn't come out of the payroll ...        1\n",
       "7163         9127  One-third of the president's budget is at the ...        1\n",
       "\n",
       "[7164 rows x 3 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def balance_classes(data):\n",
    "    # show how many data points there are for each verdict in the training data\n",
    "    print(\"Old counts:\\n\", data.groupby(\"Verdict\").count())\n",
    "    # obtain number of samples for smallest class\n",
    "    min_count = data.groupby(\"Verdict\").count()['Text'].min()\n",
    "    # sample from all classes this amount\n",
    "    class1 = data[data['Verdict'] == -1].sample(min_count)\n",
    "    class2 = data[data['Verdict'] == 0].sample(min_count)\n",
    "    class3 = data[data['Verdict'] == 1].sample(min_count)\n",
    "    # combine\n",
    "    data_balanced = pd.concat([class1, class2, class3], ignore_index=True)\n",
    "    # verify counts\n",
    "    print(\"New counts:\\n\", data_balanced.groupby(\"Verdict\").count())\n",
    "    return data_balanced\n",
    "\n",
    "train_data = balance_classes(train_data)\n",
    "train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a24091",
   "metadata": {
    "papermill": {
     "duration": 0.005379,
     "end_time": "2025-03-11T15:32:01.683939",
     "exception": false,
     "start_time": "2025-03-11T15:32:01.678560",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Tokenization, Case Folding, Stopword and Punctuation Removal\n",
    "Perform tokenization on text data:\n",
    "- make lowercase\n",
    "- remove stopwords\n",
    "- remove punctuation\n",
    "- possible to lemmatize but it is not done here.\n",
    "\n",
    "If any sentences only contain stopwords, then remove the whole row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "615c5751",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-11T15:32:01.695772Z",
     "iopub.status.busy": "2025-03-11T15:32:01.695354Z",
     "iopub.status.idle": "2025-03-11T15:32:14.407690Z",
     "shell.execute_reply": "2025-03-11T15:32:14.406570Z"
    },
    "papermill": {
     "duration": 12.720499,
     "end_time": "2025-03-11T15:32:14.409666",
     "exception": false,
     "start_time": "2025-03-11T15:32:01.689167",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence_id</th>\n",
       "      <th>Text</th>\n",
       "      <th>Verdict</th>\n",
       "      <th>Tokens</th>\n",
       "      <th>Pos</th>\n",
       "      <th>Remove</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20082</td>\n",
       "      <td>As soon as she releases them, I will release.</td>\n",
       "      <td>-1</td>\n",
       "      <td>[soon, release, release]</td>\n",
       "      <td>[ADV, VERB, VERB]</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>17364</td>\n",
       "      <td>We have to respect one another.</td>\n",
       "      <td>-1</td>\n",
       "      <td>[respect]</td>\n",
       "      <td>[VERB]</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2094</td>\n",
       "      <td>If we turn to Helsinki - I'm glad you raised i...</td>\n",
       "      <td>-1</td>\n",
       "      <td>[turn, helsinki, glad, raise, mr., uh, frankel]</td>\n",
       "      <td>[VERB, PROPN, ADJ, VERB, PROPN, INTJ, PROPN]</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8663</td>\n",
       "      <td>We have to be able to compete in the world mar...</td>\n",
       "      <td>-1</td>\n",
       "      <td>[able, compete, world, market]</td>\n",
       "      <td>[ADJ, VERB, NOUN, NOUN]</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>21552</td>\n",
       "      <td>And you look at our miners.</td>\n",
       "      <td>-1</td>\n",
       "      <td>[look, miner]</td>\n",
       "      <td>[VERB, NOUN]</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7159</th>\n",
       "      <td>18245</td>\n",
       "      <td>That was something I concurred with...</td>\n",
       "      <td>1</td>\n",
       "      <td>[concur]</td>\n",
       "      <td>[VERB]</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7160</th>\n",
       "      <td>17623</td>\n",
       "      <td>We have in the Dole-Kemp economic plan, unless...</td>\n",
       "      <td>1</td>\n",
       "      <td>[dole, kemp, economic, plan, home, worth, 500,...</td>\n",
       "      <td>[PROPN, PROPN, ADJ, NOUN, NOUN, NOUN, NUM, NOUN]</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7161</th>\n",
       "      <td>16520</td>\n",
       "      <td>Many of the countries below that used to say, ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[country, problem, demand, problem, work, coop...</td>\n",
       "      <td>[NOUN, NOUN, NOUN, NOUN, VERB, ADV, PROPN, NOUN]</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7162</th>\n",
       "      <td>9886</td>\n",
       "      <td>Of course, it doesn't come out of the payroll ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[course, come, payroll, tax]</td>\n",
       "      <td>[ADV, VERB, NOUN, NOUN]</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7163</th>\n",
       "      <td>9127</td>\n",
       "      <td>One-third of the president's budget is at the ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[president, budget, president, discretion, con...</td>\n",
       "      <td>[NOUN, NOUN, NOUN, NOUN, PROPN, VERB, NOUN, VE...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7135 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Sentence_id                                               Text  Verdict  \\\n",
       "0           20082      As soon as she releases them, I will release.       -1   \n",
       "1           17364                    We have to respect one another.       -1   \n",
       "2            2094  If we turn to Helsinki - I'm glad you raised i...       -1   \n",
       "3            8663  We have to be able to compete in the world mar...       -1   \n",
       "4           21552                        And you look at our miners.       -1   \n",
       "...           ...                                                ...      ...   \n",
       "7159        18245             That was something I concurred with...        1   \n",
       "7160        17623  We have in the Dole-Kemp economic plan, unless...        1   \n",
       "7161        16520  Many of the countries below that used to say, ...        1   \n",
       "7162         9886  Of course, it doesn't come out of the payroll ...        1   \n",
       "7163         9127  One-third of the president's budget is at the ...        1   \n",
       "\n",
       "                                                 Tokens  \\\n",
       "0                              [soon, release, release]   \n",
       "1                                             [respect]   \n",
       "2       [turn, helsinki, glad, raise, mr., uh, frankel]   \n",
       "3                        [able, compete, world, market]   \n",
       "4                                         [look, miner]   \n",
       "...                                                 ...   \n",
       "7159                                           [concur]   \n",
       "7160  [dole, kemp, economic, plan, home, worth, 500,...   \n",
       "7161  [country, problem, demand, problem, work, coop...   \n",
       "7162                       [course, come, payroll, tax]   \n",
       "7163  [president, budget, president, discretion, con...   \n",
       "\n",
       "                                                    Pos  Remove  \n",
       "0                                     [ADV, VERB, VERB]   False  \n",
       "1                                                [VERB]   False  \n",
       "2          [VERB, PROPN, ADJ, VERB, PROPN, INTJ, PROPN]   False  \n",
       "3                               [ADJ, VERB, NOUN, NOUN]   False  \n",
       "4                                          [VERB, NOUN]   False  \n",
       "...                                                 ...     ...  \n",
       "7159                                             [VERB]   False  \n",
       "7160   [PROPN, PROPN, ADJ, NOUN, NOUN, NOUN, NUM, NOUN]   False  \n",
       "7161   [NOUN, NOUN, NOUN, NOUN, VERB, ADV, PROPN, NOUN]   False  \n",
       "7162                            [ADV, VERB, NOUN, NOUN]   False  \n",
       "7163  [NOUN, NOUN, NOUN, NOUN, PROPN, VERB, NOUN, VE...   False  \n",
       "\n",
       "[7135 rows x 6 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize(data):\n",
    "    text = data[\"Text\"]\n",
    "    tokens = []\n",
    "    pos = []\n",
    "    remove = [] # if no tokens are generated, remove it later\n",
    "    for doc in nlp.pipe(text, batch_size=50):\n",
    "        t = np.array([token.lemma_.lower() for token in doc if not token.is_punct and not token.is_stop])\n",
    "        p = np.array([token.pos_ for token in doc if not token.is_punct and not token.is_stop])\n",
    "        remove.append(t.shape[0] == 0)\n",
    "        tokens.append(t)\n",
    "        pos.append(p)\n",
    "    data[\"Tokens\"], data[\"Pos\"], data[\"Remove\"] = tokens, pos, remove\n",
    "    data = data.drop(data[data[\"Remove\"]].index)\n",
    "    return data\n",
    "\n",
    "tokenized_data = tokenize(train_data)\n",
    "tokenized_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb11b0f6",
   "metadata": {
    "papermill": {
     "duration": 0.006381,
     "end_time": "2025-03-11T15:32:14.421830",
     "exception": false,
     "start_time": "2025-03-11T15:32:14.415449",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Feature Engineering\n",
    "After processing the Text into tokens, we have to derive features from the tokens. A few approaches available:\n",
    "- Bag-of-Words representation\n",
    "- Document term matrix with tf-idf weights\n",
    "- PPMI term context matrix (?)\n",
    "- Dense word embedding (Word2Vec)\n",
    "- Can also apply PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c0e35f27",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-11T15:32:14.435073Z",
     "iopub.status.busy": "2025-03-11T15:32:14.434580Z",
     "iopub.status.idle": "2025-03-11T15:32:15.208474Z",
     "shell.execute_reply": "2025-03-11T15:32:15.206900Z"
    },
    "papermill": {
     "duration": 0.782757,
     "end_time": "2025-03-11T15:32:15.210327",
     "exception": false,
     "start_time": "2025-03-11T15:32:14.427570",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7135, 5939)\n"
     ]
    }
   ],
   "source": [
    "# obtain a document vector representing each sentence\n",
    "def fit_transform(tokens):\n",
    "    return vectorizer.fit_transform(tokens).toarray()\n",
    "\n",
    "# obtain counts of adjectives, adverbs and numbers as additional features\n",
    "def compute_adj_adv_num_counts(pos):\n",
    "    result = []\n",
    "    for p in pos:\n",
    "        adj_count = np.sum(p == 'ADJ')\n",
    "        adv_count = np.sum(p == 'ADV')\n",
    "        num_count = np.sum(p == 'NUM')\n",
    "        total_count = p.shape[0]\n",
    "        result.append(np.divide(np.array([adj_count, adv_count, num_count]), total_count))\n",
    "        \n",
    "    return pd.DataFrame(result, columns=[\"ADJ\", \"ADV\", \"NUM\"])\n",
    "\n",
    "word_features = fit_transform(tokenized_data[\"Tokens\"])\n",
    "pos_features = compute_adj_adv_num_counts(tokenized_data[\"Pos\"])\n",
    "features = np.concatenate((word_features, pos_features), axis=1)\n",
    "print(features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad6bbf5",
   "metadata": {
    "papermill": {
     "duration": 0.005253,
     "end_time": "2025-03-11T15:32:15.221355",
     "exception": false,
     "start_time": "2025-03-11T15:32:15.216102",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Data Split\n",
    "Split data into training, validation, and test sets for training a model.\n",
    "We will use a 80-10-10 split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b99a615b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-11T15:32:15.233797Z",
     "iopub.status.busy": "2025-03-11T15:32:15.233425Z",
     "iopub.status.idle": "2025-03-11T15:32:15.420139Z",
     "shell.execute_reply": "2025-03-11T15:32:15.419018Z"
    },
    "papermill": {
     "duration": 0.195081,
     "end_time": "2025-03-11T15:32:15.421853",
     "exception": false,
     "start_time": "2025-03-11T15:32:15.226772",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows\n",
      "X_train:  5708 y_train:  5708\n",
      "X_valid:  713 y_valid:  713\n",
      "X_test:  714 y_test:  714\n"
     ]
    }
   ],
   "source": [
    "X, y = features, tokenized_data[\"Verdict\"]\n",
    "assert features.shape[0] == tokenized_data[\"Verdict\"].shape[0]\n",
    "\n",
    "X_train, X_a, y_train, y_a = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_valid, X_test, y_valid, y_test = train_test_split(X_a, y_a, test_size=0.5, random_state=24)\n",
    "print(\"Number of rows\")\n",
    "print(\"X_train: \", X_train.shape[0], \"y_train: \", y_train.shape[0])\n",
    "print(\"X_valid: \", X_valid.shape[0], \"y_valid: \", y_valid.shape[0])\n",
    "print(\"X_test: \", X_test.shape[0], \"y_test: \", y_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "199613f2",
   "metadata": {
    "papermill": {
     "duration": 0.005389,
     "end_time": "2025-03-11T15:32:15.433256",
     "exception": false,
     "start_time": "2025-03-11T15:32:15.427867",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Modelling\n",
    "For the model, we can choose from these 3 approaches:\n",
    "- Naive Bayes (generative classifier)\n",
    "- Logistic Regression (discriminative classifier)\n",
    "- Multi-Layer Perceptron Neural Network (discriminative classifier)\n",
    "\n",
    "To obtain a baseline model, we will only do this for now:\n",
    "- Features: Bag-of-Words, one-hot encoding of documents\n",
    "- Model: Naive Bayes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0b573308",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-11T15:32:15.445716Z",
     "iopub.status.busy": "2025-03-11T15:32:15.445296Z",
     "iopub.status.idle": "2025-03-11T15:32:15.450612Z",
     "shell.execute_reply": "2025-03-11T15:32:15.449512Z"
    },
    "papermill": {
     "duration": 0.013542,
     "end_time": "2025-03-11T15:32:15.452396",
     "exception": false,
     "start_time": "2025-03-11T15:32:15.438854",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Logistic Regression Model with tf-idf encoding of sentences.\n",
    "class Model:\n",
    "    def __init__(self):\n",
    "        self.classifier = LogisticRegression(max_iter=1000)\n",
    "    \n",
    "    def train(self, X_train, y_train):\n",
    "        self.classifier.fit(X_train, y_train)\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        return self.classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0d8e81fb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-11T15:32:15.465596Z",
     "iopub.status.busy": "2025-03-11T15:32:15.465185Z",
     "iopub.status.idle": "2025-03-11T15:32:23.337208Z",
     "shell.execute_reply": "2025-03-11T15:32:23.336151Z"
    },
    "papermill": {
     "duration": 7.880793,
     "end_time": "2025-03-11T15:32:23.339187",
     "exception": false,
     "start_time": "2025-03-11T15:32:15.458394",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# train model\n",
    "model = Model()\n",
    "model.train(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f0fdfb",
   "metadata": {
    "papermill": {
     "duration": 0.00649,
     "end_time": "2025-03-11T15:32:23.353039",
     "exception": false,
     "start_time": "2025-03-11T15:32:23.346549",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Results\n",
    "Predict results and compute performance of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c599d73c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-11T15:32:23.365628Z",
     "iopub.status.busy": "2025-03-11T15:32:23.365273Z",
     "iopub.status.idle": "2025-03-11T15:32:23.389782Z",
     "shell.execute_reply": "2025-03-11T15:32:23.388588Z"
    },
    "papermill": {
     "duration": 0.033019,
     "end_time": "2025-03-11T15:32:23.391636",
     "exception": false,
     "start_time": "2025-03-11T15:32:23.358617",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.652174</td>\n",
       "      <td>0.632184</td>\n",
       "      <td>0.642023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.567901</td>\n",
       "      <td>0.592275</td>\n",
       "      <td>0.579832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.568807</td>\n",
       "      <td>0.563636</td>\n",
       "      <td>0.566210</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Class  Precision    Recall        F1\n",
       "0   -1.0   0.652174  0.632184  0.642023\n",
       "1    0.0   0.567901  0.592275  0.579832\n",
       "2    1.0   0.568807  0.563636  0.566210"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compute_performance_per_class(model, X_test, y_test):\n",
    "    y_pred = model.predict(X_test)\n",
    "    # compute separately for each class\n",
    "    result = []\n",
    "    for c in [-1, 0, 1]:\n",
    "        TP = np.sum((y_pred == c) & (y_test == c))\n",
    "        FP = np.sum((y_pred == c) & (y_test != c))\n",
    "        FN = np.sum((y_pred != c) & (y_test == c))\n",
    "        TN = np.sum((y_pred != c) & (y_test != c))\n",
    "        precision = TP / (TP + FP)\n",
    "        recall = TP / (TP + FN)\n",
    "        F1 = 2 * (precision * recall) / (precision + recall)\n",
    "        result.append([c, precision, recall, F1])\n",
    "    return pd.DataFrame(data=np.array(result), columns=[\"Class\", \"Precision\", \"Recall\", \"F1\"])\n",
    "results = compute_performance_per_class(model, X_test, y_test)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "820f369e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-11T15:32:23.404734Z",
     "iopub.status.busy": "2025-03-11T15:32:23.404409Z",
     "iopub.status.idle": "2025-03-11T15:32:23.409859Z",
     "shell.execute_reply": "2025-03-11T15:32:23.408742Z"
    },
    "papermill": {
     "duration": 0.013893,
     "end_time": "2025-03-11T15:32:23.411405",
     "exception": false,
     "start_time": "2025-03-11T15:32:23.397512",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro F1:  0.5960217749129039\n"
     ]
    }
   ],
   "source": [
    "def compute_macro_f1(f1_scores):\n",
    "    return np.mean(f1_scores)\n",
    "\n",
    "macro_f1 = compute_macro_f1(results['F1'])\n",
    "print(\"Macro F1: \", macro_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec86fe4e",
   "metadata": {
    "papermill": {
     "duration": 0.00548,
     "end_time": "2025-03-11T15:32:23.422736",
     "exception": false,
     "start_time": "2025-03-11T15:32:23.417256",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "71aa9d99",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-11T15:32:23.435571Z",
     "iopub.status.busy": "2025-03-11T15:32:23.435185Z",
     "iopub.status.idle": "2025-03-11T15:32:23.440475Z",
     "shell.execute_reply": "2025-03-11T15:32:23.439441Z"
    },
    "papermill": {
     "duration": 0.01352,
     "end_time": "2025-03-11T15:32:23.442059",
     "exception": false,
     "start_time": "2025-03-11T15:32:23.428539",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_result(test, y_pred, filename):\n",
    "    ''' generate csv file base on the y_pred '''\n",
    "    test['Verdict'] = pd.Series(y_pred)\n",
    "    test.drop(columns=['Text'], inplace=True)\n",
    "    test.to_csv(filename, index=False)\n",
    "\n",
    "# output_filename = f\"A2_{_NAME}_{_STUDENT_NUM}.csv\"\n",
    "# generate_result(test, y_pred, output_filename)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 11087755,
     "sourceId": 93118,
     "sourceType": "competition"
    },
    {
     "datasetId": 6843835,
     "sourceId": 10994756,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30918,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 38.744552,
   "end_time": "2025-03-11T15:32:25.859172",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-03-11T15:31:47.114620",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
