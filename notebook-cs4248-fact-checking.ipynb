{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f9280d7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-11T03:10:19.495201Z",
     "iopub.status.busy": "2025-03-11T03:10:19.494756Z",
     "iopub.status.idle": "2025-03-11T03:10:19.500557Z",
     "shell.execute_reply": "2025-03-11T03:10:19.499612Z"
    },
    "papermill": {
     "duration": 0.014002,
     "end_time": "2025-03-11T03:10:19.502591",
     "exception": false,
     "start_time": "2025-03-11T03:10:19.488589",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TODO: Replace with your Student NET ID\n",
    "_NAME = \"Jason Lee Jia Xuan\"\n",
    "_STUDENT_NUM = 'E0957670'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8cefa364",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-03-11T03:10:19.512697Z",
     "iopub.status.busy": "2025-03-11T03:10:19.512369Z",
     "iopub.status.idle": "2025-03-11T03:10:21.988437Z",
     "shell.execute_reply": "2025-03-11T03:10:21.986790Z"
    },
    "papermill": {
     "duration": 2.484577,
     "end_time": "2025-03-11T03:10:21.991895",
     "exception": false,
     "start_time": "2025-03-11T03:10:19.507318",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/cs-4248-fact-checking-2420/train.csv\n",
      "/kaggle/input/cs-4248-fact-checking-2420/test.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from sklearn.metrics import f1_score\n",
    "# for tokenizing and extracting bag-of-words vectors\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f713625b",
   "metadata": {
    "papermill": {
     "duration": 0.004606,
     "end_time": "2025-03-11T03:10:22.002941",
     "exception": false,
     "start_time": "2025-03-11T03:10:21.998335",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "88f34136",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-11T03:10:22.012623Z",
     "iopub.status.busy": "2025-03-11T03:10:22.012102Z",
     "iopub.status.idle": "2025-03-11T03:10:22.135611Z",
     "shell.execute_reply": "2025-03-11T03:10:22.134493Z"
    },
    "papermill": {
     "duration": 0.130768,
     "end_time": "2025-03-11T03:10:22.137861",
     "exception": false,
     "start_time": "2025-03-11T03:10:22.007093",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence_id</th>\n",
       "      <th>Text</th>\n",
       "      <th>Verdict</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>I think we've seen a deterioration of values.</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>I think for a while as a nation we condoned th...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>For a while, as I recall, it even seems to me ...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>So we've seen a deterioration in values, and o...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>We got away, we got into this feeling that val...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Sentence_id                                               Text  Verdict\n",
       "0            1      I think we've seen a deterioration of values.       -1\n",
       "1            2  I think for a while as a nation we condoned th...       -1\n",
       "2            3  For a while, as I recall, it even seems to me ...       -1\n",
       "3            4  So we've seen a deterioration in values, and o...       -1\n",
       "4            5  We got away, we got into this feeling that val...       -1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import data\n",
    "train_data = pd.read_csv(\"../input/cs-4248-fact-checking-2420/train.csv\")\n",
    "test_data = pd.read_csv(\"../input/cs-4248-fact-checking-2420/test.csv\")\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b74dacf4",
   "metadata": {
    "papermill": {
     "duration": 0.008689,
     "end_time": "2025-03-11T03:10:22.156164",
     "exception": false,
     "start_time": "2025-03-11T03:10:22.147475",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Data Preprocessing\n",
    "Do some data preprocessing so that the data is of a good quality\n",
    "- Clean data\n",
    "- Resolve imbalances\n",
    "    - Sampling\n",
    "    - Data augmentation (?)\n",
    "- Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3264fe0b",
   "metadata": {
    "papermill": {
     "duration": 0.007192,
     "end_time": "2025-03-11T03:10:22.168095",
     "exception": false,
     "start_time": "2025-03-11T03:10:22.160903",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Clean Data\n",
    "Obtain a standardized set of data\n",
    "- Data should not contain missing values\n",
    "- Data should not have duplicates. If there are any duplicates, remove them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f9c4010",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-11T03:10:22.186702Z",
     "iopub.status.busy": "2025-03-11T03:10:22.186268Z",
     "iopub.status.idle": "2025-03-11T03:10:22.224145Z",
     "shell.execute_reply": "2025-03-11T03:10:22.222765Z"
    },
    "papermill": {
     "duration": 0.050099,
     "end_time": "2025-03-11T03:10:22.226945",
     "exception": false,
     "start_time": "2025-03-11T03:10:22.176846",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows with null Sentence_id:  0\n",
      "Rows with null Text:  0\n",
      "Rows with null Verdict:  0\n"
     ]
    }
   ],
   "source": [
    "# remove missing values and remove duplicates\n",
    "def clean_data(data):\n",
    "    # count missing data, I think kaggle tells us the data does not have missing values\n",
    "    print(\"Rows with null Sentence_id: \", sum(data[\"Sentence_id\"].isnull()))\n",
    "    print(\"Rows with null Text: \", sum(data[\"Text\"].isnull()))\n",
    "    print(\"Rows with null Verdict: \", sum(data[\"Verdict\"].isnull()))\n",
    "\n",
    "    # remove duplicates from the data\n",
    "    # set keep=False because we have no idea which label is actually correct\n",
    "    data_cleaned = data.drop_duplicates([\"Text\"], keep=False)\n",
    "    return data_cleaned\n",
    "\n",
    "train_data = clean_data(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfbf246a",
   "metadata": {
    "papermill": {
     "duration": 0.006384,
     "end_time": "2025-03-11T03:10:22.240627",
     "exception": false,
     "start_time": "2025-03-11T03:10:22.234243",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Resolve Class Imbalance\n",
    "In order to train the model properly, we need to resolve the class imbalance.\n",
    "We can either upsample or downsample.\n",
    "- For simplicity, we try downsampling here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0785e264",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-11T03:10:22.255237Z",
     "iopub.status.busy": "2025-03-11T03:10:22.254839Z",
     "iopub.status.idle": "2025-03-11T03:10:22.300453Z",
     "shell.execute_reply": "2025-03-11T03:10:22.299411Z"
    },
    "papermill": {
     "duration": 0.05617,
     "end_time": "2025-03-11T03:10:22.302243",
     "exception": false,
     "start_time": "2025-03-11T03:10:22.246073",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Old counts:\n",
      "          Sentence_id   Text\n",
      "Verdict                    \n",
      "-1             14542  14542\n",
      " 0              2388   2388\n",
      " 1              5386   5386\n",
      "New counts:\n",
      "          Sentence_id  Text\n",
      "Verdict                   \n",
      "-1              2388  2388\n",
      " 0              2388  2388\n",
      " 1              2388  2388\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence_id</th>\n",
       "      <th>Text</th>\n",
       "      <th>Verdict</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5150</td>\n",
       "      <td>His experience has been different from mine.</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4868</td>\n",
       "      <td>Now, there are people who go to bed hungry in ...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2155</td>\n",
       "      <td>It's my strong feeling that we ought to sell a...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4220</td>\n",
       "      <td>Sure, there's more work to do.</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9963</td>\n",
       "      <td>And if a few more people had gone to the polls...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7159</th>\n",
       "      <td>5566</td>\n",
       "      <td>And that state we controlled spending.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7160</th>\n",
       "      <td>9924</td>\n",
       "      <td>While I was Governor, more than eight years ag...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7161</th>\n",
       "      <td>7993</td>\n",
       "      <td>And yet even though Ambassador Smith and Ambas...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7162</th>\n",
       "      <td>18521</td>\n",
       "      <td>It's not driven by politics.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7163</th>\n",
       "      <td>5741</td>\n",
       "      <td>And we would have lost a million jobs.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7164 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Sentence_id                                               Text  Verdict\n",
       "0            5150       His experience has been different from mine.       -1\n",
       "1            4868  Now, there are people who go to bed hungry in ...       -1\n",
       "2            2155  It's my strong feeling that we ought to sell a...       -1\n",
       "3            4220                     Sure, there's more work to do.       -1\n",
       "4            9963  And if a few more people had gone to the polls...       -1\n",
       "...           ...                                                ...      ...\n",
       "7159         5566             And that state we controlled spending.        1\n",
       "7160         9924  While I was Governor, more than eight years ag...        1\n",
       "7161         7993  And yet even though Ambassador Smith and Ambas...        1\n",
       "7162        18521                       It's not driven by politics.        1\n",
       "7163         5741             And we would have lost a million jobs.        1\n",
       "\n",
       "[7164 rows x 3 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def balance_classes(data):\n",
    "    # show how many data points there are for each verdict in the training data\n",
    "    print(\"Old counts:\\n\", data.groupby(\"Verdict\").count())\n",
    "    # obtain number of samples for smallest class\n",
    "    min_count = data.groupby(\"Verdict\").count()['Text'].min()\n",
    "    # sample from all classes this amount\n",
    "    class1 = data[data['Verdict'] == -1].sample(min_count)\n",
    "    class2 = data[data['Verdict'] == 0].sample(min_count)\n",
    "    class3 = data[data['Verdict'] == 1].sample(min_count)\n",
    "    # combine\n",
    "    data_balanced = pd.concat([class1, class2, class3], ignore_index=True)\n",
    "    # verify counts\n",
    "    print(\"New counts:\\n\", data_balanced.groupby(\"Verdict\").count())\n",
    "    return data_balanced\n",
    "\n",
    "train_data = balance_classes(train_data)\n",
    "train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99fd54f3",
   "metadata": {
    "papermill": {
     "duration": 0.004463,
     "end_time": "2025-03-11T03:10:22.311642",
     "exception": false,
     "start_time": "2025-03-11T03:10:22.307179",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Data Split\n",
    "Split data into training, validation, and test sets for training a model.\n",
    "We will use a 80-10-10 split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1a576b25",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-11T03:10:22.322584Z",
     "iopub.status.busy": "2025-03-11T03:10:22.322252Z",
     "iopub.status.idle": "2025-03-11T03:10:22.335416Z",
     "shell.execute_reply": "2025-03-11T03:10:22.334048Z"
    },
    "papermill": {
     "duration": 0.020991,
     "end_time": "2025-03-11T03:10:22.337413",
     "exception": false,
     "start_time": "2025-03-11T03:10:22.316422",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows\n",
      "X_train:  5731 y_train:  5731\n",
      "X_valid:  716 y_valid:  716\n",
      "X_test:  717 y_test:  717\n"
     ]
    }
   ],
   "source": [
    "X, y = train_data[\"Text\"], train_data[\"Verdict\"]\n",
    "X_train, X_a, y_train, y_a = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_valid, X_test, y_valid, y_test = train_test_split(X_a, y_a, test_size=0.5, random_state=24)\n",
    "print(\"Number of rows\")\n",
    "print(\"X_train: \", X_train.shape[0], \"y_train: \", y_train.shape[0])\n",
    "print(\"X_valid: \", X_valid.shape[0], \"y_valid: \", y_valid.shape[0])\n",
    "print(\"X_test: \", X_test.shape[0], \"y_test: \", y_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1eda9c8a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-11T03:10:22.348497Z",
     "iopub.status.busy": "2025-03-11T03:10:22.348148Z",
     "iopub.status.idle": "2025-03-11T03:10:22.353958Z",
     "shell.execute_reply": "2025-03-11T03:10:22.353024Z"
    },
    "papermill": {
     "duration": 0.013407,
     "end_time": "2025-03-11T03:10:22.355717",
     "exception": false,
     "start_time": "2025-03-11T03:10:22.342310",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Naive Bayes Model with one-hot encoding vectors of words (Bag of Words)\n",
    "class Model:\n",
    "    def __init__(self):\n",
    "        self.vectorizer = CountVectorizer(lowercase=True)\n",
    "        self.classifier = MultinomialNB()\n",
    "    \n",
    "    def train(self, X_train, y_train):\n",
    "        # fit the vectorizer and learn the vocabulary\n",
    "        X_train_features = self.vectorizer.fit_transform(X_train).toarray()\n",
    "        # fit the classifier to learn from the extracted vectors\n",
    "        self.classifier.fit(X_train_features, y_train)\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        X_test_features = self.vectorizer.transform(X_test)\n",
    "        return self.classifier.predict(X_test_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8839d46f",
   "metadata": {
    "papermill": {
     "duration": 0.004934,
     "end_time": "2025-03-11T03:10:22.366986",
     "exception": false,
     "start_time": "2025-03-11T03:10:22.362052",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Feature Engineering and Modelling\n",
    "To engineer features, we need to firstly process the Text strings via:\n",
    "- Tokenization (compulsory)\n",
    "- Case Folding\n",
    "- Stemming\n",
    "- Lemmatization\n",
    "- Segmentation\n",
    "\n",
    "To obtain a baseline model, we will only do:\n",
    "- Tokenization\n",
    "\n",
    "After processing the Text into tokens, we have to derive features from the tokens. A few approaches available:\n",
    "- Bag-of-Words representation\n",
    "- Document term matrix with tf-idf weights\n",
    "- PPMI term context matrix (?)\n",
    "- Dense word embedding (Word2Vec)\n",
    "- Can also apply PCA\n",
    "\n",
    "For the model, we can choose from these 3 approaches:\n",
    "- Naive Bayes (generative classifier)\n",
    "- Logistic Regression (discriminative classifier)\n",
    "- Multi-Layer Perceptron Neural Network (discriminative classifier)\n",
    "\n",
    "To obtain a baseline model, we will only do this for now:\n",
    "- Features: Bag-of-Words, one-hot encoding of documents\n",
    "- Model: Naive Bayes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bb192311",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-11T03:10:22.378580Z",
     "iopub.status.busy": "2025-03-11T03:10:22.378220Z",
     "iopub.status.idle": "2025-03-11T03:10:24.603700Z",
     "shell.execute_reply": "2025-03-11T03:10:24.602463Z"
    },
    "papermill": {
     "duration": 2.233957,
     "end_time": "2025-03-11T03:10:24.605791",
     "exception": false,
     "start_time": "2025-03-11T03:10:22.371834",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# train model\n",
    "model = Model()\n",
    "model.train(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "186e0961",
   "metadata": {
    "papermill": {
     "duration": 0.004972,
     "end_time": "2025-03-11T03:10:24.615927",
     "exception": false,
     "start_time": "2025-03-11T03:10:24.610955",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Results\n",
    "Predict results and compute performance of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ecf6f545",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-11T03:10:24.627082Z",
     "iopub.status.busy": "2025-03-11T03:10:24.626680Z",
     "iopub.status.idle": "2025-03-11T03:10:24.661284Z",
     "shell.execute_reply": "2025-03-11T03:10:24.660053Z"
    },
    "papermill": {
     "duration": 0.042521,
     "end_time": "2025-03-11T03:10:24.663458",
     "exception": false,
     "start_time": "2025-03-11T03:10:24.620937",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.645522</td>\n",
       "      <td>0.670543</td>\n",
       "      <td>0.657795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.660465</td>\n",
       "      <td>0.565737</td>\n",
       "      <td>0.609442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.572650</td>\n",
       "      <td>0.644231</td>\n",
       "      <td>0.606335</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Class  Precision    Recall        F1\n",
       "0   -1.0   0.645522  0.670543  0.657795\n",
       "1    0.0   0.660465  0.565737  0.609442\n",
       "2    1.0   0.572650  0.644231  0.606335"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compute_performance_per_class(model, X_test, y_test):\n",
    "    y_pred = model.predict(X_test)\n",
    "    # compute separately for each class\n",
    "    result = []\n",
    "    for c in [-1, 0, 1]:\n",
    "        TP = np.sum((y_pred == c) & (y_test == c))\n",
    "        FP = np.sum((y_pred == c) & (y_test != c))\n",
    "        FN = np.sum((y_pred != c) & (y_test == c))\n",
    "        TN = np.sum((y_pred != c) & (y_test != c))\n",
    "        precision = TP / (TP + FP)\n",
    "        recall = TP / (TP + FN)\n",
    "        F1 = 2 * (precision * recall) / (precision + recall)\n",
    "        result.append([c, precision, recall, F1])\n",
    "    return pd.DataFrame(data=np.array(result), columns=[\"Class\", \"Precision\", \"Recall\", \"F1\"])\n",
    "results = compute_performance_per_class(model, X_test, y_test)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cc0dc1ef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-11T03:10:24.675363Z",
     "iopub.status.busy": "2025-03-11T03:10:24.674919Z",
     "iopub.status.idle": "2025-03-11T03:10:24.681401Z",
     "shell.execute_reply": "2025-03-11T03:10:24.680194Z"
    },
    "papermill": {
     "duration": 0.014321,
     "end_time": "2025-03-11T03:10:24.683055",
     "exception": false,
     "start_time": "2025-03-11T03:10:24.668734",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro F1:  0.6245238595069599\n"
     ]
    }
   ],
   "source": [
    "def compute_macro_f1(f1_scores):\n",
    "    return np.mean(f1_scores)\n",
    "\n",
    "macro_f1 = compute_macro_f1(results['F1'])\n",
    "print(\"Macro F1: \", macro_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d5ccdf7",
   "metadata": {
    "papermill": {
     "duration": 0.004724,
     "end_time": "2025-03-11T03:10:24.693026",
     "exception": false,
     "start_time": "2025-03-11T03:10:24.688302",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f5ec79bb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-11T03:10:24.704511Z",
     "iopub.status.busy": "2025-03-11T03:10:24.704126Z",
     "iopub.status.idle": "2025-03-11T03:10:24.709648Z",
     "shell.execute_reply": "2025-03-11T03:10:24.708471Z"
    },
    "papermill": {
     "duration": 0.013721,
     "end_time": "2025-03-11T03:10:24.711819",
     "exception": false,
     "start_time": "2025-03-11T03:10:24.698098",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_result(test, y_pred, filename):\n",
    "    ''' generate csv file base on the y_pred '''\n",
    "    test['Verdict'] = pd.Series(y_pred)\n",
    "    test.drop(columns=['Text'], inplace=True)\n",
    "    test.to_csv(filename, index=False)\n",
    "\n",
    "# output_filename = f\"A2_{_NAME}_{_STUDENT_NUM}.csv\"\n",
    "# generate_result(test, y_pred, output_filename)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 11087755,
     "sourceId": 93118,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30918,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 9.539962,
   "end_time": "2025-03-11T03:10:25.539126",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-03-11T03:10:15.999164",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
