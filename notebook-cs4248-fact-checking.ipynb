{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4573a00",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-13T15:32:52.129454Z",
     "iopub.status.busy": "2025-03-13T15:32:52.129079Z",
     "iopub.status.idle": "2025-03-13T15:32:52.134956Z",
     "shell.execute_reply": "2025-03-13T15:32:52.134013Z"
    },
    "papermill": {
     "duration": 0.015332,
     "end_time": "2025-03-13T15:32:52.136675",
     "exception": false,
     "start_time": "2025-03-13T15:32:52.121343",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TODO: Replace with your Student NET ID\n",
    "_NAME = \"Jason Lee Jia Xuan\"\n",
    "_STUDENT_NUM = 'E0957670'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac426bca",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-03-13T15:32:52.149803Z",
     "iopub.status.busy": "2025-03-13T15:32:52.149461Z",
     "iopub.status.idle": "2025-03-13T15:33:01.530478Z",
     "shell.execute_reply": "2025-03-13T15:33:01.529370Z"
    },
    "papermill": {
     "duration": 9.389699,
     "end_time": "2025-03-13T15:33:01.532540",
     "exception": false,
     "start_time": "2025-03-13T15:32:52.142841",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spaCy pipeline:  ['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']\n",
      "/kaggle/input/cs-4248-fact-checking-2420/train.csv\n",
      "/kaggle/input/cs-4248-fact-checking-2420/test.csv\n",
      "/kaggle/input/glove-6b/glove.6B.200d.txt\n",
      "/kaggle/input/glove-6b/glove.6B.50d.txt\n",
      "/kaggle/input/glove-6b/glove.6B.300d.txt\n",
      "/kaggle/input/glove-6b/glove.6B.100d.txt\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import math\n",
    "import time\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from sklearn.metrics import f1_score\n",
    "# for tokenizing and extracting bag-of-words vectors\n",
    "# from sklearn.feature_extraction.text import CountVectorizer\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# from sklearn.exceptions import NotFittedError\n",
    "\n",
    "# tokenizer\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "print(\"spaCy pipeline: \", nlp.pipe_names)\n",
    "\n",
    "# multilayer perceptron\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e24c597d",
   "metadata": {
    "papermill": {
     "duration": 0.005692,
     "end_time": "2025-03-13T15:33:01.544249",
     "exception": false,
     "start_time": "2025-03-13T15:33:01.538557",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae656873",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-13T15:33:01.557123Z",
     "iopub.status.busy": "2025-03-13T15:33:01.556522Z",
     "iopub.status.idle": "2025-03-13T15:33:01.691035Z",
     "shell.execute_reply": "2025-03-13T15:33:01.689950Z"
    },
    "papermill": {
     "duration": 0.142765,
     "end_time": "2025-03-13T15:33:01.692642",
     "exception": false,
     "start_time": "2025-03-13T15:33:01.549877",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence_id</th>\n",
       "      <th>Text</th>\n",
       "      <th>Verdict</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>I think we've seen a deterioration of values.</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>I think for a while as a nation we condoned th...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>For a while, as I recall, it even seems to me ...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>So we've seen a deterioration in values, and o...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>We got away, we got into this feeling that val...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Sentence_id                                               Text  Verdict\n",
       "0            1      I think we've seen a deterioration of values.       -1\n",
       "1            2  I think for a while as a nation we condoned th...       -1\n",
       "2            3  For a while, as I recall, it even seems to me ...       -1\n",
       "3            4  So we've seen a deterioration in values, and o...       -1\n",
       "4            5  We got away, we got into this feeling that val...       -1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import data\n",
    "train_data = pd.read_csv(\"../input/cs-4248-fact-checking-2420/train.csv\")\n",
    "test_data = pd.read_csv(\"../input/cs-4248-fact-checking-2420/test.csv\")\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4502723b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-13T15:33:01.705842Z",
     "iopub.status.busy": "2025-03-13T15:33:01.705507Z",
     "iopub.status.idle": "2025-03-13T15:33:34.364166Z",
     "shell.execute_reply": "2025-03-13T15:33:34.363080Z"
    },
    "papermill": {
     "duration": 32.672858,
     "end_time": "2025-03-13T15:33:34.371589",
     "exception": false,
     "start_time": "2025-03-13T15:33:01.698731",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400000 words loaded!\n",
      "time taken: 32.65091943740845\n"
     ]
    }
   ],
   "source": [
    "# import GloVe word embeddings\n",
    "glove_word_embeddings = {}\n",
    "word_embedding_dim = 300 # adjust as necessary\n",
    "with open(\"/kaggle/input/glove-6b/glove.6B.300d.txt\", 'r', encoding=\"utf-8\") as file:\n",
    "    start = time.time()\n",
    "    for line in file:\n",
    "        spl = line.split()\n",
    "        word = spl[0]\n",
    "        embedding = spl[1:]\n",
    "        glove_word_embeddings[word] = np.array(embedding, dtype=np.float64)\n",
    "    end = time.time()\n",
    "    print(f\"{len(glove_word_embeddings)} words loaded!\")\n",
    "    print(f\"time taken: {end - start}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f59c6ac0",
   "metadata": {
    "papermill": {
     "duration": 0.005893,
     "end_time": "2025-03-13T15:33:34.384667",
     "exception": false,
     "start_time": "2025-03-13T15:33:34.378774",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Data Preprocessing\n",
    "Do some data preprocessing so that the data is of a good quality\n",
    "- Clean data\n",
    "- Resolve imbalances\n",
    "    - Sampling\n",
    "    - Data augmentation (?)\n",
    "- Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ba927e",
   "metadata": {
    "papermill": {
     "duration": 0.005814,
     "end_time": "2025-03-13T15:33:34.396551",
     "exception": false,
     "start_time": "2025-03-13T15:33:34.390737",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Clean Data\n",
    "Obtain a standardized set of data\n",
    "- Data should not contain missing values\n",
    "- Data should not have duplicates. If there are any duplicates, remove them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b7eef3d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-13T15:33:34.409881Z",
     "iopub.status.busy": "2025-03-13T15:33:34.409540Z",
     "iopub.status.idle": "2025-03-13T15:33:34.415419Z",
     "shell.execute_reply": "2025-03-13T15:33:34.414093Z"
    },
    "papermill": {
     "duration": 0.014558,
     "end_time": "2025-03-13T15:33:34.417081",
     "exception": false,
     "start_time": "2025-03-13T15:33:34.402523",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# remove missing values and remove duplicates\n",
    "def clean_data(data):\n",
    "    # count missing data, I think kaggle tells us the data does not have missing values\n",
    "    print(\"Rows with null Sentence_id: \", sum(data[\"Sentence_id\"].isnull()))\n",
    "    print(\"Rows with null Text: \", sum(data[\"Text\"].isnull()))\n",
    "    print(\"Rows with null Verdict: \", sum(data[\"Verdict\"].isnull()))\n",
    "\n",
    "    # remove duplicates from the data\n",
    "    # set keep=False because we have no idea which label is actually correct\n",
    "    data_cleaned = data.drop_duplicates([\"Text\"], keep=False)\n",
    "    return data_cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59eb91d1",
   "metadata": {
    "papermill": {
     "duration": 0.005771,
     "end_time": "2025-03-13T15:33:34.429119",
     "exception": false,
     "start_time": "2025-03-13T15:33:34.423348",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Resolve Class Imbalance\n",
    "In order to train the model properly, we need to resolve the class imbalance.\n",
    "We can either upsample or downsample.\n",
    "- For simplicity, we try downsampling here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "624ecded",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-13T15:33:34.444025Z",
     "iopub.status.busy": "2025-03-13T15:33:34.443551Z",
     "iopub.status.idle": "2025-03-13T15:33:34.450109Z",
     "shell.execute_reply": "2025-03-13T15:33:34.449047Z"
    },
    "papermill": {
     "duration": 0.015972,
     "end_time": "2025-03-13T15:33:34.451834",
     "exception": false,
     "start_time": "2025-03-13T15:33:34.435862",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def balance_classes(data):\n",
    "    # show how many data points there are for each verdict in the training data\n",
    "    print(\"Old counts:\\n\", data.groupby(\"Verdict\").count())        \n",
    "    # sample from all classes this amount\n",
    "    min_count = data.groupby(\"Verdict\").count()['Text'].min()\n",
    "    class1 = data[data['Verdict'] == -1].sample(min_count, random_state=42)\n",
    "    class2 = data[data['Verdict'] == 0].sample(min_count, random_state=42)\n",
    "    class3 = data[data['Verdict'] == 1].sample(min_count, random_state=42)\n",
    "    # combine\n",
    "    data_balanced = pd.concat([class1, class2, class3], ignore_index=True)\n",
    "    # verify counts\n",
    "    print(\"New counts:\\n\", data_balanced.groupby(\"Verdict\").count())\n",
    "    return data_balanced"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f8a11a0",
   "metadata": {
    "papermill": {
     "duration": 0.0058,
     "end_time": "2025-03-13T15:33:34.463754",
     "exception": false,
     "start_time": "2025-03-13T15:33:34.457954",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Tokenization, Case Folding, Stopword and Punctuation Removal\n",
    "Perform tokenization on text data:\n",
    "- make lowercase\n",
    "- remove stopwords\n",
    "- remove punctuation\n",
    "- possible to lemmatize but it is not done here.\n",
    "\n",
    "If any sentences only contain stopwords, then remove the whole row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6e220285",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-13T15:33:34.476973Z",
     "iopub.status.busy": "2025-03-13T15:33:34.476647Z",
     "iopub.status.idle": "2025-03-13T15:33:34.483283Z",
     "shell.execute_reply": "2025-03-13T15:33:34.482393Z"
    },
    "papermill": {
     "duration": 0.015052,
     "end_time": "2025-03-13T15:33:34.484765",
     "exception": false,
     "start_time": "2025-03-13T15:33:34.469713",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tokenize(data):\n",
    "    result = data.copy()\n",
    "    text = result[\"Text\"]\n",
    "    tokens = []\n",
    "    pos = []\n",
    "    remove = [] # if no tokens are generated, remove it later\n",
    "    content = []\n",
    "    for doc in nlp.pipe(text, batch_size=50):\n",
    "        # remove tokens that we don't want\n",
    "        doc_clean = [token for token in doc]\n",
    "        # tokenize\n",
    "        t = np.array([token.lower_ for token in doc_clean])\n",
    "        p = np.array([token.pos_ for token in doc_clean])\n",
    "        x = np.array([token.lemma_.lower() for token in doc_clean if not token.is_stop]) # lemmatized, stopword removed token list\n",
    "        remove.append(t.shape[0] == 0)\n",
    "        tokens.append(t)\n",
    "        content.append(x)\n",
    "        pos.append(p)\n",
    "\n",
    "    result[\"Tokens\"] = tokens\n",
    "    result[\"Pos\"] = pos\n",
    "    result[\"Remove\"] = remove\n",
    "    result[\"Content\"] = content\n",
    "    result = result.drop(result[result[\"Remove\"]].index)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4281f7f1",
   "metadata": {
    "papermill": {
     "duration": 0.005688,
     "end_time": "2025-03-13T15:33:34.496560",
     "exception": false,
     "start_time": "2025-03-13T15:33:34.490872",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Data Split\n",
    "Split data into training, validation, and test sets for training a model.\n",
    "We will use a 80-10-10 split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8d718dba",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-13T15:33:34.510553Z",
     "iopub.status.busy": "2025-03-13T15:33:34.509981Z",
     "iopub.status.idle": "2025-03-13T15:33:34.518606Z",
     "shell.execute_reply": "2025-03-13T15:33:34.517470Z"
    },
    "papermill": {
     "duration": 0.017785,
     "end_time": "2025-03-13T15:33:34.520366",
     "exception": false,
     "start_time": "2025-03-13T15:33:34.502581",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def split_data(tokenized_data):\n",
    "    # split by class first\n",
    "    class1 = tokenized_data[tokenized_data[\"Verdict\"] == -1]\n",
    "    class2 = tokenized_data[tokenized_data[\"Verdict\"] == 1]\n",
    "    class3 = tokenized_data[tokenized_data[\"Verdict\"] == 0]\n",
    "    \n",
    "    # split tokenized data into 80-20 split before engineering features\n",
    "    count80 = math.floor(class1.shape[0] * 0.8)\n",
    "    count20 = class1.shape[0] - count80\n",
    "    sample1a = class1.sample(count80, random_state=10)\n",
    "    sample1b = class1.drop(sample1a.index)\n",
    "    count80 = math.floor(class2.shape[0] * 0.8)\n",
    "    count20 = class2.shape[0] - count80\n",
    "    sample2a = class2.sample(count80, random_state=10)\n",
    "    sample2b = class2.drop(sample2a.index)\n",
    "    count80 = math.floor(class3.shape[0] * 0.8)\n",
    "    count20 = class3.shape[0] - count80\n",
    "    sample3a = class3.sample(count80, random_state=10)\n",
    "    sample3b = class3.drop(sample3a.index)\n",
    "    tokenized_train = pd.concat((sample1a, sample2a, sample3a), axis=0)\n",
    "    tokenized_test = pd.concat((sample1b, sample2b, sample3b), axis=0)\n",
    "    \n",
    "    y_train = tokenized_train[\"Verdict\"]\n",
    "    y_test = tokenized_test[\"Verdict\"]\n",
    "    return tokenized_train, tokenized_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27736819",
   "metadata": {
    "papermill": {
     "duration": 0.005724,
     "end_time": "2025-03-13T15:33:34.532604",
     "exception": false,
     "start_time": "2025-03-13T15:33:34.526880",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Feature Engineering\n",
    "After processing the Text into tokens, we have to derive features from the tokens. A few approaches available:\n",
    "- Bag-of-Words representation\n",
    "- Document term matrix with tf-idf weights\n",
    "- PPMI term context matrix (?)\n",
    "- Dense word embedding (Word2Vec)\n",
    "- Can also apply PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3cbe71fa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-13T15:33:34.546241Z",
     "iopub.status.busy": "2025-03-13T15:33:34.545862Z",
     "iopub.status.idle": "2025-03-13T15:33:34.554977Z",
     "shell.execute_reply": "2025-03-13T15:33:34.554042Z"
    },
    "papermill": {
     "duration": 0.017711,
     "end_time": "2025-03-13T15:33:34.556608",
     "exception": false,
     "start_time": "2025-03-13T15:33:34.538897",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def count(doc, words):\n",
    "    count = 0\n",
    "    for token in doc:\n",
    "        if token in words:\n",
    "            count += 1\n",
    "    return count\n",
    "\n",
    "def count_ref_other_people(doc):\n",
    "    words = set([\"you\", \"others\", \"people\", \"yourself\", \"yourselves\", \"your\", \"\"])\n",
    "    return count(doc, words)\n",
    "\n",
    "def count_ref_self(doc):\n",
    "    words = set([\"i\", \"self\", \"me\", \"myself\", \"mine\", \"friends\", \"friend\", \"family\", \"buddy\", \"mate\"])\n",
    "    return count(doc, words)\n",
    "\n",
    "def count_causal(doc):\n",
    "    words = set([\"cause\", \"because\", \"effect\", \"hence\", \"therefore\", \"thus\", \"since\", \"reason\", \"due\"])\n",
    "    return count(doc, words)\n",
    "\n",
    "def count_negation(doc):\n",
    "    words = set([\"no\", \"not\", \"neither\", \"none\", \"nobody\", \"nothing\", \"nowhere\", \"hardly\", \"seldom\", \"little\"])\n",
    "    return count(doc, words)\n",
    "\n",
    "def count_promise(doc):\n",
    "    words = set([\"\\'ll\", \"will\", \"should\", \"future\", \"believe\", \"think\", \"consider\", \"propose\", \"want\", \"suspect\", \"suppose\", \"time\", \"come\", \"upcoming\"])\n",
    "    return count(doc, words)\n",
    "\n",
    "def count_all_or_nothing(doc):\n",
    "    words = set([\"everything\", \"nothing\", \"everyone\", \"all\", \"no\", \"never\", \"always\", ])\n",
    "    return count(doc, words)\n",
    "\n",
    "def count_prosocial(doc):\n",
    "    words = set([\"care\", \"help\", \"please\", \"thank\", \"thanks\", \"support\", \"trust\", \"faith\"])\n",
    "    return count(doc, words)\n",
    "\n",
    "def count_together(doc):\n",
    "    words = set([\"we\", \"us\", \"our\", \"ours\", \"together\"])\n",
    "    return count(doc, words)\n",
    "\n",
    "def count_accusation(doc):\n",
    "    words = set([\"lie\", \"steal\", \"cheat\", \"betray\", \"deceive\", \"manipulate\", \"harm\", \"ruin\", \"destroy\", \"fake\", \"liar\", \"cheater\", \"fraud\", \"backstabber\", \"traitor\", \"thief\", \"hypocrite\", \"coward\", \"fool\", \"idiot\", \"moron\"])\n",
    "    return count(doc, words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3c17ac56",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-13T15:33:34.569873Z",
     "iopub.status.busy": "2025-03-13T15:33:34.569557Z",
     "iopub.status.idle": "2025-03-13T15:33:34.577152Z",
     "shell.execute_reply": "2025-03-13T15:33:34.576130Z"
    },
    "papermill": {
     "duration": 0.016437,
     "end_time": "2025-03-13T15:33:34.579103",
     "exception": false,
     "start_time": "2025-03-13T15:33:34.562666",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# obtain an embedding vector representing each sentence by taking sum over all word embeddings in each sentence\n",
    "def compute_embeddings(corpus):\n",
    "    doc_embeddings = []\n",
    "    for doc in corpus:\n",
    "        doc_embedding = np.zeros(word_embedding_dim, dtype=np.float64)\n",
    "        for token in doc:\n",
    "            if token in glove_word_embeddings:\n",
    "                doc_embedding = np.add(doc_embedding, glove_word_embeddings[token])\n",
    "        doc_embeddings.append(doc_embedding)\n",
    "    return np.array(doc_embeddings)\n",
    "\n",
    "# def compute_tfidf(tokens, vectorizer):\n",
    "#     try:\n",
    "#         vectorizer.transform(tokens)\n",
    "#     except NotFittedError as e:\n",
    "#         print(\"fitting vectorizer!\")\n",
    "#         vectorizer.fit(tokens)\n",
    "\n",
    "#     return vectorizer.transform(tokens).toarray()\n",
    "\n",
    "# compute counts of certain words\n",
    "def get_additional_features(corpus):\n",
    "    result = []\n",
    "    for doc in corpus:\n",
    "        counts = [\n",
    "            count_ref_other_people(doc),\n",
    "            count_ref_self(doc),\n",
    "            count_causal(doc),\n",
    "            count_negation(doc),\n",
    "            count_promise(doc),\n",
    "            count_all_or_nothing(doc),\n",
    "            count_prosocial(doc),\n",
    "            count_together(doc),\n",
    "        ]\n",
    "        result.append(counts)\n",
    "    return np.array(result)\n",
    "\n",
    "# some counts need lemmatized vocabulary\n",
    "def get_additional_features2(lemmas):\n",
    "    result = []\n",
    "    for doc in lemmas:\n",
    "        counts = [\n",
    "            count_accusation(doc)\n",
    "        ]\n",
    "        result.append(counts)\n",
    "    return np.array(result)\n",
    "\n",
    "# def conduct_pca(features, pca):\n",
    "#     try:\n",
    "#         pca.transform(features)\n",
    "#     except NotFittedError as e:\n",
    "#         print(\"fitting pca!\")\n",
    "#         pca.fit(features)\n",
    "#         print(\"PCA cumulative variance: \", np.cumsum(pca.explained_variance_ratio_))\n",
    "#     return pca.transform(features)\n",
    "    \n",
    "\n",
    "# pipeline which outputs all features, run with train data first\n",
    "def get_features(tokenized_data):\n",
    "    features = [\n",
    "        compute_embeddings(tokenized_data[\"Tokens\"]),\n",
    "        get_additional_features(tokenized_data[\"Tokens\"]),\n",
    "        get_additional_features2(tokenized_data[\"Content\"]),\n",
    "    ]\n",
    "    features = np.concatenate(features, axis=1)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c8e176",
   "metadata": {
    "papermill": {
     "duration": 0.005698,
     "end_time": "2025-03-13T15:33:34.590951",
     "exception": false,
     "start_time": "2025-03-13T15:33:34.585253",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Modelling\n",
    "For the model, we can choose from these 3 approaches:\n",
    "- Naive Bayes (generative classifier)\n",
    "- Logistic Regression (discriminative classifier)\n",
    "- Multi-Layer Perceptron Neural Network (discriminative classifier)\n",
    "\n",
    "To obtain a baseline model, we will only do this for now:\n",
    "- Features: Bag-of-Words, one-hot encoding of documents\n",
    "- Model: Naive Bayes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2472e31a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-13T15:33:34.604038Z",
     "iopub.status.busy": "2025-03-13T15:33:34.603681Z",
     "iopub.status.idle": "2025-03-13T15:33:34.609317Z",
     "shell.execute_reply": "2025-03-13T15:33:34.608435Z"
    },
    "papermill": {
     "duration": 0.013942,
     "end_time": "2025-03-13T15:33:34.610820",
     "exception": false,
     "start_time": "2025-03-13T15:33:34.596878",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Neural Network works on word embeddings of sentence and other features\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, feature_dim):\n",
    "        super().__init__()\n",
    "        # define multilayer perceptron layers\n",
    "        self.fc1 = nn.Linear(feature_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, 32)\n",
    "        self.fc3 = nn.Linear(32, 3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        # x = F.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a8bba3",
   "metadata": {
    "papermill": {
     "duration": 0.005785,
     "end_time": "2025-03-13T15:33:34.623021",
     "exception": false,
     "start_time": "2025-03-13T15:33:34.617236",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Results\n",
    "Predict results and compute performance of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a40ad236",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-13T15:33:34.636337Z",
     "iopub.status.busy": "2025-03-13T15:33:34.635966Z",
     "iopub.status.idle": "2025-03-13T15:33:34.643621Z",
     "shell.execute_reply": "2025-03-13T15:33:34.642425Z"
    },
    "papermill": {
     "duration": 0.01625,
     "end_time": "2025-03-13T15:33:34.645164",
     "exception": false,
     "start_time": "2025-03-13T15:33:34.628914",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_performance_per_class(model, X_test, y_test):\n",
    "    # y_pred = model.predict(X_test)\n",
    "    pred_model = model(X_test)\n",
    "    _, y_pred = pred_model.max(1)\n",
    "    # need to convert values of 2 in y_pred back to -1\n",
    "    y_pred = y_pred.numpy()\n",
    "    y_test = y_test.numpy()\n",
    "    y_pred[y_pred == 2] = -1\n",
    "    y_test[y_test == 2] = -1\n",
    "    print(y_pred)\n",
    "    print(y_pred.shape)\n",
    "    print(y_test.shape)\n",
    "    print(y_pred[:10])\n",
    "    # compute separately for each class\n",
    "    result = []\n",
    "    for c in [-1, 0, 1]:\n",
    "        TP = np.sum((y_pred == c) & (y_test == c))\n",
    "        FP = np.sum((y_pred == c) & (y_test != c))\n",
    "        FN = np.sum((y_pred != c) & (y_test == c))\n",
    "        TN = np.sum((y_pred != c) & (y_test != c))\n",
    "        precision = TP / (TP + FP)\n",
    "        recall = TP / (TP + FN)\n",
    "        F1 = 2 * (precision * recall) / (precision + recall)\n",
    "        result.append([c, precision, recall, F1])\n",
    "    return pd.DataFrame(data=np.array(result), columns=[\"Class\", \"Precision\", \"Recall\", \"F1\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "57872f94",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-13T15:33:34.658655Z",
     "iopub.status.busy": "2025-03-13T15:33:34.658312Z",
     "iopub.status.idle": "2025-03-13T15:33:34.663290Z",
     "shell.execute_reply": "2025-03-13T15:33:34.662040Z"
    },
    "papermill": {
     "duration": 0.013489,
     "end_time": "2025-03-13T15:33:34.664851",
     "exception": false,
     "start_time": "2025-03-13T15:33:34.651362",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_macro_f1(f1_scores):\n",
    "    return np.mean(f1_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6dac68c9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-13T15:33:34.678127Z",
     "iopub.status.busy": "2025-03-13T15:33:34.677741Z",
     "iopub.status.idle": "2025-03-13T15:33:34.684426Z",
     "shell.execute_reply": "2025-03-13T15:33:34.683258Z"
    },
    "papermill": {
     "duration": 0.014952,
     "end_time": "2025-03-13T15:33:34.685932",
     "exception": false,
     "start_time": "2025-03-13T15:33:34.670980",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_performance_per_class_2(model, X_test, y_test):\n",
    "    y_pred = model.predict(X_test)\n",
    "    # compute separately for each class\n",
    "    result = []\n",
    "    for c in [-1, 0, 1]:\n",
    "        TP = np.sum((y_pred == c) & (y_test == c))\n",
    "        FP = np.sum((y_pred == c) & (y_test != c))\n",
    "        FN = np.sum((y_pred != c) & (y_test == c))\n",
    "        TN = np.sum((y_pred != c) & (y_test != c))\n",
    "        precision = TP / (TP + FP)\n",
    "        recall = TP / (TP + FN)\n",
    "        F1 = 2 * (precision * recall) / (precision + recall)\n",
    "        result.append([c, precision, recall, F1])\n",
    "    return pd.DataFrame(data=np.array(result), columns=[\"Class\", \"Precision\", \"Recall\", \"F1\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "421e52a9",
   "metadata": {
    "papermill": {
     "duration": 0.006667,
     "end_time": "2025-03-13T15:33:34.698956",
     "exception": false,
     "start_time": "2025-03-13T15:33:34.692289",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Run Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2d8ec36a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-13T15:33:34.713150Z",
     "iopub.status.busy": "2025-03-13T15:33:34.712793Z",
     "iopub.status.idle": "2025-03-13T15:33:34.762539Z",
     "shell.execute_reply": "2025-03-13T15:33:34.761405Z"
    },
    "papermill": {
     "duration": 0.058381,
     "end_time": "2025-03-13T15:33:34.764128",
     "exception": false,
     "start_time": "2025-03-13T15:33:34.705747",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows with null Sentence_id:  0\n",
      "Rows with null Text:  0\n",
      "Rows with null Verdict:  0\n",
      "Old counts:\n",
      "          Sentence_id   Text\n",
      "Verdict                    \n",
      "-1             14542  14542\n",
      " 0              2388   2388\n",
      " 1              5386   5386\n",
      "New counts:\n",
      "          Sentence_id  Text\n",
      "Verdict                   \n",
      "-1              2388  2388\n",
      " 0              2388  2388\n",
      " 1              2388  2388\n"
     ]
    }
   ],
   "source": [
    "# clean data\n",
    "cleaned_data = clean_data(train_data)\n",
    "# balanced data\n",
    "balanced_data = balance_classes(cleaned_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6269a183",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-13T15:33:34.777979Z",
     "iopub.status.busy": "2025-03-13T15:33:34.777633Z",
     "iopub.status.idle": "2025-03-13T15:33:57.565396Z",
     "shell.execute_reply": "2025-03-13T15:33:57.564246Z"
    },
    "papermill": {
     "duration": 22.797084,
     "end_time": "2025-03-13T15:33:57.567423",
     "exception": false,
     "start_time": "2025-03-13T15:33:34.770339",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized data columns:  Index(['Sentence_id', 'Text', 'Verdict', 'Tokens', 'Pos', 'Remove', 'Content'], dtype='object')\n",
      "X_train:  (5730, 309)\n"
     ]
    }
   ],
   "source": [
    "# tokenize data\n",
    "tokenized_data = tokenize(balanced_data)\n",
    "print(\"Tokenized data columns: \", tokenized_data.columns)\n",
    "# split data\n",
    "tokenized_train, tokenized_test, y_train, y_test = split_data(tokenized_data)\n",
    "# engineer features\n",
    "X_train = get_features(tokenized_train)\n",
    "print(\"X_train: \", X_train.shape)\n",
    "X_test = get_features(tokenized_test)\n",
    "# set feature size\n",
    "feature_dim = X_train.shape[1]\n",
    "model = Model(feature_dim)\n",
    "\n",
    "\n",
    "# convert data to tensors\n",
    "y_train_mlp = y_train.replace(to_replace=-1, value=2)\n",
    "y_test_mlp = y_test.replace(to_replace=-1, value=2)\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train_mlp.to_numpy(), dtype=torch.long)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test_mlp.to_numpy(), dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b1767c62",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-13T15:33:57.581755Z",
     "iopub.status.busy": "2025-03-13T15:33:57.581368Z",
     "iopub.status.idle": "2025-03-13T15:35:40.132738Z",
     "shell.execute_reply": "2025-03-13T15:35:40.131616Z"
    },
    "papermill": {
     "duration": 102.560784,
     "end_time": "2025-03-13T15:35:40.134886",
     "exception": false,
     "start_time": "2025-03-13T15:33:57.574102",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.3168, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.8989, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.8397, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.8116, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7928, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7775, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7651, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# train neural network\n",
    "\n",
    "# define model parameters\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
    "\n",
    "num_epochs = 7000\n",
    "for n in range(num_epochs):\n",
    "    model.train()\n",
    "    y_pred = model(X_train_tensor)\n",
    "    loss = loss_fn(y_pred, y_train_tensor)\n",
    "    if n % 1000 == 0:\n",
    "        print(loss)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6c9419c7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-13T15:35:40.150378Z",
     "iopub.status.busy": "2025-03-13T15:35:40.149505Z",
     "iopub.status.idle": "2025-03-13T15:35:40.166609Z",
     "shell.execute_reply": "2025-03-13T15:35:40.165514Z"
    },
    "papermill": {
     "duration": 0.026545,
     "end_time": "2025-03-13T15:35:40.168398",
     "exception": false,
     "start_time": "2025-03-13T15:35:40.141853",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1 -1 -1 ...  1  0  0]\n",
      "(1434,)\n",
      "(1434,)\n",
      "[-1 -1 -1 -1 -1  0 -1  0  0 -1]\n",
      "   Class  Precision    Recall        F1\n",
      "0   -1.0   0.661710  0.744770  0.700787\n",
      "1    0.0   0.659389  0.631799  0.645299\n",
      "2    1.0   0.703196  0.644351  0.672489\n",
      "Macro F1:  0.6728585432811269\n"
     ]
    }
   ],
   "source": [
    "# compute results\n",
    "results = compute_performance_per_class(model, X_test_tensor, y_test_tensor)\n",
    "print(results)\n",
    "macro_f1 = compute_macro_f1(results['F1'])\n",
    "print(\"Macro F1: \", macro_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea387c6",
   "metadata": {
    "papermill": {
     "duration": 0.006304,
     "end_time": "2025-03-13T15:35:40.181326",
     "exception": false,
     "start_time": "2025-03-13T15:35:40.175022",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "664b28d5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-13T15:35:40.196960Z",
     "iopub.status.busy": "2025-03-13T15:35:40.196602Z",
     "iopub.status.idle": "2025-03-13T15:35:43.223125Z",
     "shell.execute_reply": "2025-03-13T15:35:43.221740Z"
    },
    "papermill": {
     "duration": 3.037053,
     "end_time": "2025-03-13T15:35:43.224851",
     "exception": false,
     "start_time": "2025-03-13T15:35:40.187798",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1032, 2)\n",
      "Tokenized data columns:  Index(['Sentence_id', 'Text', 'Tokens', 'Pos', 'Remove', 'Content'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "def generate_result(test, y_pred, filename):\n",
    "    print(np.array(test).shape)\n",
    "    tokenized_data = tokenize(test)\n",
    "    print(\"Tokenized data columns: \", tokenized_data.columns)\n",
    "    X_test = get_features(tokenized_data)\n",
    "    X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "\n",
    "    model.eval()\n",
    "    pred_model = model(X_test_tensor)\n",
    "    _, y_pred = pred_model.max(1)\n",
    "    y_pred = y_pred.numpy()\n",
    "    y_pred[y_pred == 2] = -1\n",
    "    \n",
    "    ''' generate csv file base on the y_pred '''\n",
    "    test['Verdict'] = pd.Series(y_pred)\n",
    "    test.drop(columns=['Text'], inplace=True)\n",
    "    test.to_csv(filename, index=False)\n",
    "\n",
    "output_filename = f\"A2_{_NAME}_{_STUDENT_NUM}.csv\"\n",
    "generate_result(test_data, y_pred, output_filename)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 11087755,
     "sourceId": 93118,
     "sourceType": "competition"
    },
    {
     "datasetId": 6847560,
     "sourceId": 10999989,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30918,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 176.663807,
   "end_time": "2025-03-13T15:35:46.043554",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-03-13T15:32:49.379747",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
